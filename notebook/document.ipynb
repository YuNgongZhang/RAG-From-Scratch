{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27fcbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Document Structure\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10ad5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'page': 1, 'author': 'John Doe', 'date_created': '2025-11-19'}, page_content='This is the content of the document. I am using to create RAG')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(page_content=\"This is the content of the document. I am using to create RAG\",\n",
    "               metadata={\"source\": \"example.txt\", \"page\": 1, \"author\": \"John Doe\", \"date_created\": \"2025-11-19\"})\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0745d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a simple txt file\n",
    "\n",
    "import os\n",
    "os.makedirs('../data/text_files', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e8ac48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d114fdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\fyp\\rag-from-scratch\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "###text loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f63da74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"../data/text_files\", glob=\"*.txt\", loader_cls= TextLoader, loader_kwargs={\"encoding\":\"utf-8\"}, show_progress=False)\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec601d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 0}, page_content='LECTURE 6: CONTEXT-FREE GRAMMARS\\n1. Structured Programming\\n2. Beyond Regular languages\\n3. The Parser Phase\\n4. Context-free grammars (CFG)\\n5. Chomsky Hierarchy'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 1}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 2}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 3}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 4}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 5}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 6}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 7}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 8}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 9}, page_content='ENTER THE DRAGON\\nAKA STRUCTURED PROGRAMMING\\nStructured programming replaces goto with scoped control structure.\\nControl Structures: recursive programming language constructs, like\\nAn EXPR is\\nWhy do we need \\n?\\nif E XP R then E XP R else E XP R f i\\nwhile E XP R loop E XP R pool\\n⋯\\nloop ⋯pool\\nhttps://en.wikipedia.org/wiki/Structured_programming'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 10}, page_content='SCOPE\\nDefinition [Scope]\\nBinding associates a name with an entity, like a variable to a value, as in the assignment\\nstatement x := 5. The scope of a name binding is the part of a program where the name\\nbinding is valid, that is where the name can be used to refer to the entity.\\nScope defines a namespace, a set of bindings visible in the same part of a program.\\nKeeping namespaces small and isolated is key to understanding and maintaining software;\\nit enables data encapsulation.\\nhttps://en.wikipedia.org/wiki/Scope_(computer_science)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 11}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 12}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 13}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 14}, page_content='PUSHBACK\\nComputability\\nEfficiency\\nExpressivity'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 15}, page_content='STRUCTURED PROGRAMMING THEOREM\\nDefinition [Böhm–Jacopini Theorem, 1966]\\nA class of control flow graphs (historically called flowcharts in this context) can compute\\nany computable function if it combines subprograms in only three specific ways (control\\nstructures). These are\\nExecuting one subprogram, and then another subprogram (sequence)\\nExecuting one of two subprograms according to the value of a boolean expression\\n(selection)\\nRepeatedly executing a subprogram as long as a boolean expression is true (iteration)\\nA flowchart subject to these constraints may, however, use additional variables to track\\ninformation that the original program represents by the program location.\\nhttps://en.wikipedia.org/wiki/Structured_program_theorem'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 16}, page_content='STRUCTURED PROGRAMMING THEOREM\\nDefinition [Böhm–Jacopini Theorem, 1966]\\nA class of control flow graphs (historically called flowcharts in this context) can compute\\nany computable function if it combines subprograms in only three specific ways (control\\nstructures). These are\\nExecuting one subprogram, and then another subprogram (sequence)\\nExecuting one of two subprograms according to the value of a boolean expression\\n(selection)\\nRepeatedly executing a subprogram as long as a boolean expression is true (iteration)\\nA flowchart subject to these constraints may, however, use additional variables to track\\ninformation that the original program represents by the program location.\\nThis addresses computability.\\nhttps://en.wikipedia.org/wiki/Structured_program_theorem'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 17}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 18}, page_content='EFFICENCY: DEVELOPER PRODUCTIVITY\\nAnalysis\\nDesign\\nCoding\\nTesting\\nDocumentation\\nAssembly\\nC\\n3 weeks\\n3 weeks\\n5 weeks\\n5 weeks\\n8 weeks\\n4 weeks\\n10 weeks\\n6 weeks\\n2 weeks\\n2 weeks\\nSize\\nEffort\\nProductivity\\nAssembly\\nC\\n5000 lines\\n1500 lines\\n28 weeks\\n20 weeks\\n714 lines/month\\n300 lines/month\\nSoftware Engineering, Ian Sommerville, 9th Ed.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 19}, page_content='EFFICENCY: GENERATED CODE\\nEarly compilers of the 1960s were often primarily concerned with\\nsimply compiling code correctly or efficiently, such that compile\\ntimes were a major concern. … By the late 1980s, optimizing\\ncompilers were sufficiently effective that programming in\\nassembly language declined.\\nhttps://en.wikipedia.org/wiki/Optimizing_compiler'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 20}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 21}, page_content='WE NEED TREES\\nNot a Christmas Tree.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 22}, page_content='WE NEED TREES\\nNot Trees in the Amazon.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 23}, page_content='WE NEED TREES\\nAbstract Syntax Trees (AST).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 24}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 25}, page_content=\"BALANCED DELIMITERS (PARENTHESIS)\\nDefinition [Dyck Language]\\nFor \\n, the Dyck language is \\n all prefixes of  contain no more ')'s\\nthan '('s and the number of '('s in  equals the number of ')'s .\\nΣ = {‘(’, ‘)’}\\n{u ∈Σ ∗∣\\nu\\nu\\n}\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 26}, page_content=\"BALANCED DELIMITERS (PARENTHESIS)\\nDefinition [Dyck Language]\\nFor \\n, the Dyck language is \\n all prefixes of  contain no more ' 's than '\\n's and the number of ' 's in  equals the number of ' 's .\\nΣ = {a, b}\\n{u ∈Σ ∗∣\\nu\\nb\\na\\na\\nu\\nb }\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 27}, page_content='BEYOND REGULAR LANGUAGES\\nTo recap, structured programming \\n trees \\n balanced delimiters \\n the Dyck language\\nThe Dyck language of balanced parentheses is not regular\\n⇔\\n⇔\\n='),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 28}, page_content='WHAT CAN REGULAR LANGUAGES EXPRESS?\\nLanguages that count modulo a fixed integer\\nIntuition: A finite automation that runs long enough must repeat states\\nA finite automaton cannot remember the number of times it has visited a particular state'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 29}, page_content='PUMPING LEMMA\\nDefinition [Pumping Lemma for Regular Languages]\\nLet  be a regular language. Then there exists an integer \\n such that every string\\n of length at least  (the \"pumping length\") can be written as \\n satisfying\\nthe following conditions:\\nThe substring  can be pumped.\\nL\\np ≥1\\nw ∈L\\np\\nw = xyz\\n|y| ≥1\\n|xy| ≤p\\nxynz ∈L, ∀n ≥0\\ny\\nhttps://en.wikipedia.org/wiki/Pumping_lemma_for_regular_languages'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 30}, page_content=\"PUMPING LEMMA APPLIED TO A DYCK SUB-LANGUAGE\\n over the alphabet \\n is not regular.\\nAssume, for purposes of contradiction, that  is regular. Let \\n. Observe that\\n and \\n, by definition. By the pumping lemma, there must be some\\ndecomposition \\n with \\n and \\n such that \\n.\\nWe picked a \\n so that  contains only ' ', because \\n. Moreover, because \\n, it\\ncontains at least one ' '.\\nWe now pump up : \\n has more instances of the delimiter ' ' than the delimiter ' ',\\nsince we have added some instances of ' ' without adding instances of ' '.\\nTherefore, \\n. We have reached a contradiction. Hence,  is not regular.\\nL = {(n)n : n ≥0}\\nΣ = {(, )}\\nL\\nw = (p)p\\nw ∈L\\n|w| = 2p > p\\nw = xyz\\n|xy| ≤p\\n|y| ≥1\\nxynz ∈L, ∀n ≥0\\nw\\ny\\n(\\n|xy| ≤p\\n|y| ≥1\\n(\\ny xy2z\\n(\\n)\\n(\\n)\\nxy2z ∉L\\nL\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 31}, page_content='ENTER PARSING STAGE LEFT\\nParsers can recognise Dyck languages, and therefore the trees they encode, enabling\\nstructured programming.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 32}, page_content='A TOKEN REMINDER\\nDefinition [Token, Take 3]\\nA token pairs a token type and a lexeme.\\nUnder this definition, the statement x := y; gives the token stream ID(x) ASSIGN\\nID(y) SEMI.\\nThis lecture uses this definition of token throughout.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 33}, page_content='THE FUNCTIONALITY OF THE PARSER\\nInput\\na sequence of tokens from the lexer\\nOutput\\nparse tree of the program'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 34}, page_content='EXAMPLE\\nA code snippet: if x = y then 1 else 2 fi (aka Lexer input)\\nParser Input (aka Lexer output)\\nIF ID(x) EQ ID(y) THEN INT(1) ELSE INT(2) FI\\nParser Output'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 35}, page_content='COMPARISON WITH LEXICAL ANALYSIS\\nPhase\\nInput\\nOutput\\nLexer\\nSequence of symbols\\nSequence of tokens\\nParser\\nSequence of tokens\\nParse tree'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 36}, page_content='THE ROLE OF THE PARSER\\nNot all token sequences are programs\\nThe parser must distinguish between valid and invalid token sequences\\nWe need\\nA notation for describing for describing valid token streams\\nA method for distinguishing valid from invalid token streams'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 37}, page_content='RECALL STRUCTURED PROGRAMMING\\nProgramming language constructs are recursive.\\nAn EXPR is\\nContext-free grammars are a natural notation for this recursive structure\\nif E XP R then E XP R else E XP R f i\\nwhile E XP R loop E XP R pool\\n⋯'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 38}, page_content='CONTEXT-FREE GRAMMAR\\nDefinition (Context-free Grammar)\\nA context-free grammar is the -tuple \\n, where\\n1. \\n is a finite set of nonterminals.\\n2.  is a finite set of terminals; it is the alphabet of \\n.\\n3. \\n4. \\n is the start symbol.\\n5. \\n is the finite relation called productions or (rewrite) rules.\\nAlthough \\n has a distinguished state symbol , each nonterminal can be viewed as the\\nstart symbol of a subgrammar of \\n.\\nExample\\n4\\nG = (N , T, S, R)\\nN\\nT\\nL(G)\\nN ∩T = ∅∧ϵ∉N ∪T\\nS ∈N\\nR = N →(N ∪T)∗\\nG\\nS\\nG\\nE →\\xa0ϵ\\nE →\\xa0Y1Y2 ⋯Yn, where\\xa0E ∈N \\xa0and\\xa0Yi ∈N ∪T ∪{ϵ}'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 39}, page_content='NOTATIONAL CONVENTIONS\\nNonterminals are written in uppercase\\nTerminals are either punctuation or written in lowercase\\nVariables that range over both nonterminals and terminals are rewritten in uppercase.\\nThe start symbol is the left-hand side of the first production'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 40}, page_content='READING PRODUCTIONS\\nThis production means \\n can be replaced by \\n.\\nIt also tells us \\n, from the definition of \\n.\\nX →Y1 ⋯Yn\\nX\\nY1 ⋯Yn\\nX ∈N ∧Yi ∈(N ∪T)∗\\nR'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 41}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nE →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id\\nS ='),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 42}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nE →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id\\nS = E'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 43}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nE →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id\\nS = E\\nN = {E }'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 44}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nE →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id\\nS = E\\nN = {E }\\nT = {∗, + , (, ), id}'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 45}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nE →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id\\nS = E\\nN = {E }\\nT = {′∗′,′+ ′,′(′,′)′, id}'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 46}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nBut you are humans who like concision and can handle ambiguity.\\nE →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id\\nS = E\\nN = {E }\\nT = {∗, + , (, ), id}'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 47}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nE →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id\\nS = E\\nN = {E }\\nT = {∗, + , (, ), id}\\nR ='),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 48}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nIn other words, \\n is often sufficient to define a CFG.\\nE →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id\\nS = E\\nN = {E }\\nT = {∗, + , (, ), id}\\nR = above\\nR'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 49}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nS = E\\nN = {E }\\nT = {∗, + , (, ), id}\\nR = {E →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id}\\nS →'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 50}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nS = E\\nN = {E }\\nT = {∗, + , (, ), id}\\nR = {E →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id}\\nS →E + E →'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 51}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nS = E\\nN = {E }\\nT = {∗, + , (, ), id}\\nR = {E →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id}\\nS →E + E →\\n–'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 52}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nS = E\\nN = {E }\\nT = {∗, + , (, ), id}\\nR = {E →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id}\\nS →E + E →E + (E ) →⋯\\n–'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 53}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nS = E\\nN = {E }\\nT = {∗, + , (, ), id}\\nR = {E →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id}\\nS →E\\nE →E\\nE\\n→⋯\\n+\\n+ (\\n)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 54}, page_content='TERMINALS\\nTerminals are so-called because there are no rules for replacing them, they appear only on\\nthe right-hand side of rules.\\nOnce generated, they are permanent\\nTerminals are the tokens of \\n; indeed, the Lexer can be seen as converting a character\\nstream into a terminal stream of a grammar\\nL(G)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 55}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nS = E\\nN = {E }\\nT = {∗, + , (, ), id}\\nR = {E →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id}\\nS →E + E →E + (E ) →E + (id) →id + (id)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 56}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nEvery string in the sequence of repeated applications of \\n is a sentential form.\\nS = E\\nN = {E }\\nT = {∗, + , (, ), id}\\nR = {E →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id}\\nS →E + E →E + (E ) →E + (id) →id + (id)\\nR'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 57}, page_content='SIMPLE ARITHMETIC EXPRESSIONS\\nEvery string in the sequence of repeated applications of \\n is a sentential form. \\nis a special sentential form, composed solely of terminals, called a sentence.\\nS = E\\nN = {E }\\nT = {∗, + , (, ), id}\\nR = {E →\\xa0E ∗E\\n∣\\xa0E + E\\n∣\\xa0(E )\\n∣\\xa0id}\\nS →E + E →E + (E ) →E + (id) →\\nid + (id)\\nR\\nid + (id)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 58}, page_content='THE LANGUAGE OF A CFG\\nFor the grammar \\n with start symbol , \\n is\\nWe write \\n to denote \\n. Each \\n, with the possible\\nexception of , is a sentence. So, ignoring , \\n is the set of all sentences the grammar\\ncan generate.\\nG\\nS L(G)\\n{w ∈T ∗∣S\\n∗→w}\\nx\\n∗→y\\nx = x1 →x2 →⋯xn = y\\nw\\nϵ\\nϵ L(G)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 59}, page_content='BALANCED PARENTHESES (DYCK)\\nOne grammar, three typesettings:\\nS →\\xa0(S) S\\nS →\\xa0ϵ\\nS →\\xa0(S) S\\n∣\\xa0ϵ\\nS →(S) S ∣ϵ'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 60}, page_content='The CFG notion is a big step. Keep in mind\\n1. Membership in a language is “yes” or “no”\\n2. In addition to acceptance, a compiler must produce the parse tree of an input\\n3. Compilers should, but often do not, handle errors gracefully\\n4. We need tools, like \\n or \\n, that convert a CFG into an implementation that\\ndecides membership (#1) and builds a parse tree.\\nCUP\\ntree-sitter'),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 61}, page_content=\"The form of a grammar, its rules, is important\\nMany grammars generate the same language\\nTools, like CUP or tree-sitter, are sensitive to the grammar\\nTools for regular languages, like lex generators, like flex or JFlex (tools that we're not using\\nthis term), are, of course, also sensitive to the form of a regular expression — think of the\\n“now, you have two problems” aphorism — but they tend to go wrong silently rather than\\ncomplaining upfront, like parser generators.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m132', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36', 'creationdate': '2025-01-29T12:20:38+00:00', 'source': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Context-free Grammars.pdf', 'total_pages': 63, 'format': 'PDF 1.4', 'title': 'Context-free Grammars', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-29T12:20:38+00:00', 'trapped': '', 'modDate': \"D:20250129122038+00'00'\", 'creationDate': \"D:20250129122038+00'00'\", 'page': 62}, page_content='UP NEXT: CHOMSKY HIERACHY AND AMBIGUITY\\n1. Generating Strings for a Grammar\\n2. Chomsky Hierarchy\\n3. Derivations\\n4. Ambiguity\\nSpeaker notes'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\cv_yungongzhang_1311.pdf', 'file_path': '..\\\\data\\\\pdf\\\\cv_yungongzhang_1311.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'cv_yungongzhang.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='YuNgong Zhang (Peter) \\nPhone: +44 (0)7925 681985  | Email: yungongzhangpeter@gmail.com \\nlinkedin: https://www.linkedin.com/in/yungong-zhang-994b3b254/ \\n \\nEDUCATION \\nUniversity College London (UCL), MSc in Strategic Management of Projects\\u200b \\u200b\\n               \\u200b\\nSep 2025 – Current \\nUniversity College London (UCL), BSc in Computer Science\\u200b\\u200b\\n               \\u200b\\n\\u200b\\n\\u200b\\nSep 2022 – May 2025 \\n●\\u200b\\nAwarded 2:1 Class Honours \\n●\\u200b\\nKey Courses: Compilers, Internet of Things, Connected Systems,Object-Oriented Programming, Algorithms, Software \\nEngineering, Image Processing, Machine Learning, Discrete Mathematics, Theory of Computation, Systems Engineering \\nHarrow international School Shanghai HISS\\u200b\\n\\u200b\\n\\u200b\\n\\u200b\\n\\u200b\\n\\u200b\\n \\u200b\\n  Sep 2019 – Jul 2021 \\n●\\u200b\\nHISS results (A-level): 2 (A*) in Math and Mandarin, 2 (A) in Further Maths and Computer Science \\n \\nWORK EXPERIENCE \\nUnity – Intern (Program Tester & Programmer)\\u200b\\n\\u200b\\n\\u200b\\n            \\u200b            Jul 2019 – Sep 2019, Jul 2023 – Sep 2023 \\n●\\u200b\\nIdentified and documented 50+ bugs for Unity’s K12 coding tutorial platform, accelerating deployment by one month. \\n●\\u200b\\nOptimized Multiplayer Play Mode (MPPM) with improved Netcode compatibility and better performance on constrained \\nhardware. \\n●\\u200b\\nConducted a comparative study of Unreal’s networking architecture to inform Unity’s collaborative dev strategy. \\n●\\u200b\\nCollaborated across QA, engineering, and research teams on multiplayer and collaborative tool development. \\n \\nTECHNICAL & PROJECT EXPERIENCE \\nArtifact Detection Pipeline (Collaboration with Chinese Academy of Sciences), Developer,    \\u200b \\u200b\\n Oct 2025 – Present \\n●\\u200b\\nDesigning a scalable CV + multimodal model pipeline for artifact detection in 50k–100k-pixel Cryo-EM images, \\nusing FFT, Sobel/Gabor filters, and heatmap-based ROI extraction. \\n●\\u200b\\nImplementing a token-efficient prompting workflow by routing small models for coarse filtering and larger \\nvision-language models (e.g., GPT-4o-class) for fine-grained defect reasoning. \\n●\\u200b\\nCollaborating with CAS researchers to refine artifact taxonomy and integrate the system into real Cryo-EM QA \\nworkflows. \\n●\\u200b\\nCurrently on track for a Nature Communications submission as part of the collaborative research output. \\nNPU Adaptation (with CAS),  Developer\\u200b \\u200b\\n \\u200b\\n\\u200b\\n\\u200b\\n\\u200b\\n\\u200b\\n\\u200b\\nOct 2025 – Present \\n●\\u200b\\nRe-implementing AreTomo3’s MotionCor motion-correction module from C++/CUDA into Python to enable \\ncross-platform deployment and simplify integration with downstream Cryo-EM pipelines. \\n●\\u200b\\nWorking with CAS and Huawei engineers to validate numerical consistency, resolve low-level device-operation \\ndifferences, and prepare the system for future deployment once the Ascend Python package is fully supported. \\n●\\u200b\\nCurrently on track for a Nature Communications submission as part of the collaborative research output. \\nPrice Modelling (UCL MSc Project, with Supervisor Collaboration), Researcher\\u2003\\u2003\\u200b\\u200b\\n\\u200b\\nOct 2025 – Present \\n●\\u200b\\nDeveloping a comparative machine-learning pipeline to evaluate modelling strategies for large-scale housing price \\nprediction. \\n●\\u200b\\nExploring Random Forest, Gradient Boosting, and MLP-based architectures, focusing on feature engineering, \\nleakage-free preprocessing, and cross-validated evaluation. \\n●\\u200b\\nWorking directly with a UCL faculty supervisor to refine methodological choices and integrate results into an ongoing \\nresearch investigation. \\nFinal Year Project (UCL) – AI Prompt Compression for Debugging LLMs\\u200b \\u200b\\n\\u200b\\n\\u200b\\n Oct 2024 – Apr 2025 \\n●\\u200b\\nIndependently developed a prompt compression framework for SWE-bench debugging tasks, reducing prompt size from \\n~15–20k tokens to a fraction while maintaining patch fidelity and debugging success rates. \\n●\\u200b\\nDesigned and implemented the most efficient debugging template for SWE-agent, analyzing how LLMs prioritize \\ninformation during code repair. \\n●\\u200b\\nInvented a novel two-stage prompt method, using smaller models for compression before delegating to large models, \\nensuring equivalent debugging outcomes at significantly lower cost. \\n●\\u200b\\nReduced token usage by ~80% while maintaining debugging success, substantially lowering costs for LLM-based \\nworkflows. \\nSystems Engineering Project (UCL), Team Member,\\u200b\\n\\u200b\\n\\u200b\\n\\u200b\\n              \\u200b\\u200b\\nJan 2024 – Apr 2024 \\n●\\u200b\\nLed 80% of development and 30% of UI design to adapt motion-input software for Minecraft, enabling disabled users to \\nplay without a keyboard. \\n●\\u200b\\nImplemented OpenCV features and custom configs, and gathered user feedback from disabled participants to refine \\naccessibility. \\n●\\u200b\\nHelped the 3-person team deliver a functional system with a final grade of 70/100. \\nVR development in Unity Bootcamp, Product Manager,\\u200b\\n\\u200b\\n\\u200b\\n\\u200b\\n                                Jul 2021 – Sep 2021 \\n●\\u200b\\nBuilt a VR experience highlighting the social challenges of depression, implementing interactive scenes with Unity’s \\nPhysics engine and Event System. \\n●\\u200b\\nLed a 6-person team, managed workflow, and delivered the final demo with positive mentor feedback. \\nSKILLS AND INTERESTS \\n●\\u200b\\nProgramming: Python, Java, C#, JavaScript, Haskell \\n●\\u200b\\nFrameworks & Tools: Unity Engine, OpenCV, SWE-agent, Git \\n●\\u200b\\nCurrently Learning: C++, AI application engineering with LLMs (via AI Builders by Superlinear), focusing on \\nintegrating LLMs with external tools, debugging/evaluating outputs, and building end-to-end AI systems.  \\n●\\u200b\\nCertifications: Unity Game Developer \\n●\\u200b\\nLanguages: English, Mandarin \\n●\\u200b\\nInterests: Gaming, Basketball, Photography'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 0}, page_content='AI Prompt Compression\\nYuNgong Zhang\\nBSc Computer Science\\nSubmission Date: 30th April 2025\\nSupervisors: Earl Barr and Sergey Mechtaev\\nThis report is submitted as part requirement for the BSc Degree in\\nComputer Science at UCL. It is substantially the result of my own\\nwork except where explicitly indicated in the text. The report may be\\nfreely copied and distributed provided the source is explicitly\\nacknowledged.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 1}, page_content='Abstract\\nIn this study, we evaluate the performance of various existing prompt compression tech-\\nniques using Swebench as a standardized benchmark.\\nAdditionally, we propose and\\nimplement our own prompt compression method, comparing its efficacy against state-\\nof-the-art approaches. Our primary objective is to systematically identify redundant or\\nineffective segments within prompts that can be safely discarded without compromising\\nthe performance of AI language models. Through comparative analysis, we aim to pro-\\nvide insights into optimizing prompts for improved efficiency and accuracy in language\\nmodel applications.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 2}, page_content='Contents\\n1\\nIntroduction\\n3\\n2\\nBackground Information and Related Work\\n4\\n2.1\\nMotivation and Context\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n2.2\\nSWE-bench and SWE-Agent . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n2.2.1\\nSWE-bench: A Comprehensive Benchmark for Real-World Soft-\\nware Engineering Tasks . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n2.2.2\\nSWE-agent and SeaView: Analyzing and Visualizing LLM Behavior\\n5\\n2.3\\nThe Problem: Efficiency and Cost Challenges\\n. . . . . . . . . . . . . . .\\n6\\n2.4\\nPrompt Compression in LLMs . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n2.4.1\\nWhat is Prompt Compression?\\n. . . . . . . . . . . . . . . . . . .\\n7\\n2.4.2\\nSignificance of Prompt Compression . . . . . . . . . . . . . . . . .\\n7\\n2.4.3\\nDetailed Techniques and Critical Evaluations . . . . . . . . . . . .\\n7\\n2.4.4\\nLimitations and Challenges of Prompt Compression Techniques\\n.\\n8\\n2.4.5\\nComparative Analysis with Existing Methods\\n. . . . . . . . . . .\\n8\\n3\\nApproach and Methodology\\n10\\n3.1\\nPrompt Engineering Approach . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n3.2\\nPrompt Compression Strategies . . . . . . . . . . . . . . . . . . . . . . .\\n11\\n3.3\\nMinimal Information Principle . . . . . . . . . . . . . . . . . . . . . . . .\\n11\\n3.3.1\\nPrompt-Compression Template and Sentinel Design . . . . . . . .\\n11\\n4\\nImplementation Details and Challenges\\n13\\n4.1\\nComprehensive Cost Analysis with Full Iterative Cycle . . . . . . . . . .\\n13\\n4.1.1\\nDataset-wide Integrity Audit . . . . . . . . . . . . . . . . . . . . .\\n15\\n4.2\\nBlack-box Model Behavior . . . . . . . . . . . . . . . . . . . . . . . . . .\\n16\\n4.2.1\\nMotivating Example: Silent Patch Failure\\n. . . . . . . . . . . . .\\n17\\n4.2.2\\nEmpirical Evidence and Impact on Workflow Efficiency . . . . . .\\n17\\n4.2.3\\nMitigation Strategies . . . . . . . . . . . . . . . . . . . . . . . . .\\n17\\n4.3\\nStrict Formatting and Output Constraints . . . . . . . . . . . . . . . . .\\n17\\n4.3.1\\nChallenges Encountered\\n. . . . . . . . . . . . . . . . . . . . . . .\\n18\\n4.3.2\\nConcrete Examples . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n4.3.3\\nMitigation Strategies . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n4.3.4\\nBenchmark Data Version Mismatch . . . . . . . . . . . . . . . . .\\n19\\n4.3.5\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n4.4\\nIterative vs. One-shot Debugging Cost Analysis . . . . . . . . . . . . . .\\n20\\n4.4.1\\nDetailed Cost Analysis of Iterative Debugging (GPT-4o) . . . . .\\n20\\n4.4.2\\nDetailed Cost Analysis of One-shot Debugging (Qwen2.5) . . . . .\\n20\\n4.4.3\\nComparative Cost-benefit Analysis\\n. . . . . . . . . . . . . . . . .\\n20\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 3}, page_content='5\\nExperimental Evaluation and Analysis\\n22\\n5.1\\nEvaluation Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\n5.2\\nCase Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\n5.2.1\\nImpact of Function Context Completeness on Patch Accuracy (Is-\\nsue astropy-12907)\\n. . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\n5.2.2\\nSuccessful vs. Failing Compression in ndarithmetic.py (Issue astropy-\\n14995) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n25\\n5.2.3\\nInstruction Ignoring by Small Model (Issue 14995) . . . . . . . . .\\n26\\n5.3\\nEffect of Markdown Formatting on Instruction Sensitivity . . . . . . . . .\\n27\\n5.4\\nUnexpected Language Output from Qwen2.5-14B-Instruct\\n. . . . . . . .\\n29\\n5.5\\nEnvironment Setup and Reproducibility\\n. . . . . . . . . . . . . . . . . .\\n30\\n5.5.1\\nDirectory layout . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n30\\n5.5.2\\nStep-by-step reproduction . . . . . . . . . . . . . . . . . . . . . .\\n31\\n5.6\\nObservations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n5.6.1\\nKey Elements for Successful Prompt Compression . . . . . . . . .\\n33\\n5.6.2\\nCompression Strategy Trade-offs . . . . . . . . . . . . . . . . . . .\\n33\\n5.6.3\\nModel-Specific Behaviors and Recommendations . . . . . . . . . .\\n33\\n5.6.4\\nImplications for Real-world Debugging Workflows . . . . . . . . .\\n34\\n5.6.5\\nSummary of Observations\\n. . . . . . . . . . . . . . . . . . . . . .\\n34\\n6\\nDiscussion\\n36\\n6.1\\nManual Prompt–Compression vs. Autonomous Agent Debugging . . . . .\\n36\\n6.2\\nSemantic Clarity Improves Success Rate\\n. . . . . . . . . . . . . . . . . .\\n37\\n6.3\\nCost & Latency Trade-offs Revisited\\n. . . . . . . . . . . . . . . . . . . .\\n37\\n6.4\\nPractical Guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n37\\n6.5\\nLimitations\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n6.6\\nTakeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n7\\nFuture Work\\n39\\n7.1\\nLearning–Based Importance Scoring . . . . . . . . . . . . . . . . . . . . .\\n39\\n7.2\\nDynamic k-Beam Compression . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n7.3\\nTighter Feedback Loops with In-Context Evaluation . . . . . . . . . . . .\\n39\\n7.4\\nRobust Dataset Versioning . . . . . . . . . . . . . . . . . . . . . . . . . .\\n40\\n7.5\\nCross-Repository Generalisation . . . . . . . . . . . . . . . . . . . . . . .\\n40\\n7.6\\nEvaluating the Qwen 3 Model Family . . . . . . . . . . . . . . . . . . . .\\n40\\nReferences\\n42\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 4}, page_content='Chapter 1\\nIntroduction\\nThe explosive growth of large language models (LLMs) has opened up new possibilities for\\nautomated code repair, but also raised concerns about computational cost and prompt\\ndesign efficiency.\\nIn this project, we propose an end-to-end pipeline that leverages a\\nsmall LLM for prompt compression, a large LLM for patch generation, and the Swebench\\nbenchmark for automated evaluation. By stripping away redundant context before call-\\ning the expensive large model, our approach aims to preserve—or even improve—repair\\nquality while substantially reducing overall resource consumption.\\nConcretely, our pipeline consists of the following stages:\\n1. Data Extraction: Retrieve buggy code examples from the Swebench dataset.\\n2. Prompt Assembly: Attach a custom prompt template to each example and collect\\nthem into a single text file.\\n3. Prompt Compression: Input the assembled file into a small LLM that removes\\nunnecessary or repetitive information, yielding a concise prompt.\\n4. Patch Generation: Feed the compressed prompts into a large LLM to produce\\ncandidate repair patches.\\n5. Automated Evaluation: Submit the generated patches back to Swebench, run\\nthe test suite, and measure pass rates and performance metrics.\\nOur main contributions are:\\n• Prompt Compression Benchmarking: We systematically evaluate how differ-\\nent small LLMs perform at prompt compression and how this impacts downstream\\nrepair efficacy.\\n• Efficiency Gains: We quantify reductions in token usage and wall-clock time,\\ndemonstrating that concise prompts can match or exceed uncompressed baselines\\nin repair accuracy.\\n• Reproducible Workflow: All data, scripts, and execution instructions are pub-\\nlished in a public repository to enable straightforward replication and extension of\\nour results.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 5}, page_content='Chapter 2\\nBackground Information and Related\\nWork\\nIn this chapter, we elaborate on the motivation for this research by examining the foun-\\ndational concepts, key datasets, existing techniques, and current challenges associated\\nwith prompt compression for Large Language Models (LLMs) in debugging scenarios.\\nWe provide in-depth discussions and critical evaluations of existing literature, setting the\\nstage for our contributions.\\n2.1\\nMotivation and Context\\nRecent advancements in Large Language Models (LLMs) like GPT-4 have significantly\\nimpacted various domains, including automated software debugging. However, despite\\ntheir effectiveness, practical deployment faces substantial challenges primarily due to\\nhigh computational costs and processing inefficiencies stemming from long prompts. This\\nissue becomes particularly problematic when dealing with extensive codebases or iterative\\ndebugging workflows, necessitating solutions for efficient prompt handling.\\n2.2\\nSWE-bench and SWE-Agent\\n2.2.1\\nSWE-bench: A Comprehensive Benchmark for Real-World\\nSoftware Engineering Tasks\\nSWE-bench [1] is a rigorously constructed benchmark specifically created to evaluate the\\ncapabilities of Large Language Models (LLMs) in addressing authentic software engineer-\\ning issues from real-world scenarios. Introduced by Jimenez et al. (2023), SWE-bench\\nconsists of 2,294 carefully selected issue–pull request pairs sourced from 12 prominent\\nopen-source Python repositories hosted on GitHub. Each instance in SWE-bench pro-\\nvides comprehensive information designed to replicate real-world debugging environments\\naccurately:\\n• Issue Descriptions: Authentic issue reports sourced directly from GitHub, clearly\\noutlining bugs, feature requests, or enhancements.\\n• Codebase Snapshots: Precise repository states captured at the time the respec-\\ntive issues were originally reported, ensuring models interact with realistic and\\nunaltered code conditions.\\n• Reference Solutions: Verified human-authored pull requests that resolve each\\nissue, including detailed code modifications and associated unit tests designed to\\nvalidate the correctness of implemented solutions.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 6}, page_content='The evaluation process employed by SWE-bench involves presenting LLMs with both\\nthe issue description and corresponding codebase snapshot. Models must then generate\\nappropriate code patches intended to resolve the described issues. The efficacy of gener-\\nated patches is objectively determined by applying these patches to the original codebase\\nand executing provided unit tests to verify functional correctness and compatibility with\\nthe project’s intended behavior.\\nSWE-bench significantly advances the state of benchmarking for automated software\\ndebugging, providing notable advantages over previous evaluation methodologies:\\n• Realism: Unlike artificially constructed datasets that often isolate functions or\\nprovide contrived debugging scenarios, SWE-bench challenges models with realistic,\\nmulti-file issues reflective of everyday software engineering tasks.\\n• Diversity: By encompassing a wide variety of issues—including simple bug fixes,\\ncomplex logic errors, and extensive feature implementations—the benchmark en-\\nsures robust evaluation across a broad spectrum of real-world use cases.\\n• Reproducibility: The inclusion of exact codebase snapshots and clearly defined\\ntest suites ensures reliable, repeatable assessments, facilitating meaningful compar-\\nisons between different LLM approaches and prompt engineering strategies.\\nDespite these strengths, SWE-bench also presents several limitations identified through\\nrecent evaluations, which users must consider when interpreting benchmarking results:\\n• Solution Leakage: Approximately 32.67% of successful patches were found to\\nleverage information explicitly available in issue descriptions or associated com-\\nments, potentially inflating perceived model effectiveness.\\n• Weak Test Coverage: Around 31.08% of accepted solutions succeeded due to\\ninadequately stringent tests that failed to thoroughly verify the correctness or ro-\\nbustness of patches.\\n• Outdated Information: Given the dynamic nature of open-source repositories,\\nsome issues and associated codebases may no longer represent current coding prac-\\ntices or actively maintained code, reducing the contemporary relevance of certain\\nevaluations.\\nTo address these concerns and enhance the reliability of SWE-bench as a benchmark-\\ning standard, additional refinements such as SWE-bench Verified—a rigorously validated\\nsubset with human-reviewed correctness—and SWE-bench Multimodal, integrating vi-\\nsual or GUI-based issues, have been introduced.\\n2.2.2\\nSWE-agent and SeaView: Analyzing and Visualizing LLM\\nBehavior\\nSWE-agent provides a sophisticated environment designed explicitly for the deploy-\\nment, evaluation, and iterative improvement of LLM-driven software engineering agents.\\nIt enables models to directly interact with realistic codebases, interpret issue require-\\nments effectively, and propose precise code modifications. SWE-agent supports multiple\\nadvanced functionalities critical for practical LLM debugging workflows:\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 7}, page_content='• Environment Interaction: Models can explore and modify files, execute code\\nsegments, and run comprehensive test suites within a controlled yet realistic coding\\nenvironment, closely simulating human developer interactions.\\n• Tool Integration: Supports the integration of external analysis tools to enhance\\nmodel reasoning, code understanding, and transformation capabilities.\\n• Iterative Reasoning: Facilitates multi-step reasoning processes whereby mod-\\nels iteratively refine their solutions based on continuous environmental feedback,\\nsignificantly improving debugging accuracy over single-pass solutions.\\nTo further enhance interpretability and debugging of LLM-driven agents, the visual\\nanalytics system SeaView [2] has been developed. SeaView offers a comprehensive vi-\\nsualization platform enabling researchers and developers to:\\n• Visualize Agent Trajectories: Clearly observe the step-by-step actions taken by\\nagents, including code edits, decisions, and test outcomes, providing critical insight\\ninto model reasoning and effectiveness.\\n• Experimental Comparison: Quickly compare agent behaviors across multiple\\nexperimental conditions, configurations, or different prompt engineering methods,\\ngreatly facilitating systematic performance assessments.\\n• Diagnose Errors and Failures: Effectively pinpoint and understand the root\\ncauses of agent errors or unsuccessful attempts through detailed execution logs,\\nvisual tracing, and interactive debugging interfaces.\\nTogether, SWE-agent and SeaView significantly streamline the development, evalu-\\nation, and enhancement processes for LLM-based software engineering agents, reducing\\nboth human effort and computational resources required for rigorous model experimen-\\ntation and optimization.\\n2.3\\nThe Problem: Efficiency and Cost Challenges\\nThe deployment of Large Language Models (LLMs) such as GPT-4.1 in software de-\\nbugging tasks introduces significant efficiency and cost challenges, primarily due to their\\ntoken-based API pricing structures. For instance, OpenAI’s GPT-4.1 model charges $30\\nper million prompt tokens and $60 per million sampled tokens [3].\\nIn practical scenarios, debugging a medium-sized codebase—comprising approximately\\n8,000 tokens per prompt—over multiple iterations can rapidly accumulate substantial to-\\nken usage. For example, five debugging iterations would result in over 40,000 tokens,\\nleading to significant expenses and increased latency. These challenges are exacerbated\\nin complex debugging workflows that require detailed prompts, including extensive code\\nsnippets, error messages, and contextual information. Such comprehensive prompts not\\nonly increase token consumption but also prolong processing times, making real-time\\ndebugging impractical and costly.\\nPrompt compression emerges as a viable solution to these challenges. Techniques like\\nLLMLingua, developed by Microsoft, have demonstrated the ability to compress prompts\\nby up to 20 times without compromising the quality of the model’s responses [4]. By\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 8}, page_content='eliminating redundant or less informative content, prompt compression reduces token\\nusage, thereby lowering costs and enhancing processing efficiency.\\nFurthermore, advanced methods such as Task-Aware Prompt Compression Optimiza-\\ntion with Reinforcement Learning (TACO-RL) have been proposed to optimize prompt\\ncompression by considering task-specific information, leading to improved performance\\nacross various applications [5].\\nIn summary, as LLMs become integral to software debugging, addressing efficiency\\nand cost challenges through prompt compression is essential.\\nImplementing effective\\nprompt compression strategies not only reduces operational costs but also enhances the\\nscalability and practicality of LLMs in real-world debugging scenarios.\\n2.4\\nPrompt Compression in LLMs\\n2.4.1\\nWhat is Prompt Compression?\\nPrompt compression is the process of systematically reducing the number of tokens in\\nthe input provided to a Large Language Model (LLM) while retaining the essential\\ninformation required for accurate responses.\\nThis technique aims to optimize model\\nefficiency by minimizing computational resources and costs associated with processing\\nlengthy prompts [6].\\n2.4.2\\nSignificance of Prompt Compression\\nThe importance of prompt compression can be summarized as follows:\\n• Cost Efficiency: Reducing token usage directly lowers API expenses under token-\\nbased pricing models [6].\\n• Improved Processing Speeds: Shorter prompts result in faster inference and\\nlower latency, facilitating more rapid debugging iterations.\\n• Scalability Improvements: Compact prompts enable high-throughput and con-\\ncurrent interactions with LLM services, making deployment in resource-constrained\\nenvironments feasible.\\n2.4.3\\nDetailed Techniques and Critical Evaluations\\nMultiple strategies have been proposed and empirically validated for prompt compression:\\n• Heuristic-based Methods: Apply domain-specific rules to trim non-essential\\ncontent (e.g., comments, boilerplate code). While simple to implement, these meth-\\nods may remove context crucial for accurate debugging [7].\\n• Semantic Embedding Methods: Use sentence-level embeddings to identify and\\nretain the most semantically relevant segments. Effective but computationally in-\\ntensive due to embedding generation [8].\\n• Symbolic Optimization Approaches: Represent prompts in symbolic or pro-\\ngrammatic forms, enabling algorithmic detection of redundancy. Offers fine-grained\\ncontrol but requires specialized parsing infrastructure [9].\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 9}, page_content='• Small-Model Assisted Compression: Employ a smaller, cost-efficient LLM to\\npreprocess and compress prompts before invoking a larger model. For example,\\nLLMLingua uses a 7B-parameter model to remove redundant tokens, achieving up\\nto 20× compression with negligible accuracy loss [4].\\nAs shown in Figure 2.1, the small-model compression step reduces prompt length from\\nover 3,600 tokens to approximately 520 tokens in our experiments; detailed methodology\\nand results are presented in Section 3.2.\\nFigure 2.1: Example of prompt length before and after small-model compression.\\n2.4.4\\nLimitations and Challenges of Prompt Compression Tech-\\nniques\\nDespite its advantages, existing prompt compression methods face several challenges. Li\\net al. (2024) note that hard-prompt compression methods may inadvertently remove crit-\\nical contextual information, while soft-prompt methods often require additional training\\nresources, limiting their practical applicability [10].\\nFrom an information-theoretic perspective, Nagle et al. (2024) introduced a rate-\\ndistortion framework for prompt compression, revealing theoretical limits to the extent\\nprompts can be compressed without losing semantic integrity or reducing output qual-\\nity [11]. Consequently, finding an optimal balance between compression ratio, semantic\\nintegrity, and inference efficiency remains a significant ongoing research challenge.\\n2.4.5\\nComparative Analysis with Existing Methods\\nSeveral prompt compression frameworks have emerged, each with distinct methodologies\\nand performance characteristics:\\n• LLMLingua Series: Employs a coarse-to-fine token pruning approach and Transformer-\\nbased token classification to achieve significant token reduction with minimal per-\\nformance degradation [12, 13].\\n• TACO-RL: Uses reinforcement learning for task-specific prompt optimization,\\nachieving notable improvements in performance across various tasks [14].\\n• LLM-DCP: Frames prompt compression as a dynamic token-pruning process based\\non Markov Decision Processes, dynamically retaining only task-critical informa-\\ntion [15].\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 10}, page_content='Each method presents trade-offs between compression efficiency, inference cost, and\\nsemantic integrity, necessitating careful consideration when selecting suitable techniques\\nfor specific application scenarios.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 11}, page_content='Chapter 3\\nApproach and Methodology\\nIn this chapter, we detail our methodological framework, including the prompt engi-\\nneering principles derived from visualization insights, specific strategies used for prompt\\ncompression, and our adherence to the minimal information principle. Finally, we de-\\nscribe the structure and reproducibility of our repository.\\n3.1\\nPrompt Engineering Approach\\nInitially, we provided comprehensive raw data from the SWE-bench dataset directly to a\\nLarge Language Model (LLM), assuming it could independently localize and rectify bugs.\\nPreliminary experiments indicated significant challenges with this approach, as the LLM\\nconsistently struggled with bug localization from unstructured inputs. Insights gained\\nthrough the SWE-Agent visualization tool showed that models significantly benefit from\\nstructured and explicitly targeted prompts.\\nUsing SWE-Agent, we analyzed the LLM’s step-by-step reasoning trajectory on is-\\nsue 12907. This visualization clarified that effective prompts should explicitly preserve\\nand highlight certain key elements:\\n• Complete Function Code (including comments and formatting):\\nPreserve the entire function definition verbatim,\\nincluding all docstrings, inline comments, and formatting.\\n• Precise Bug Localization and Explanation:\\nExplicitly identify the problematic line and clearly explain\\nwhy it is incorrect, to guide accurate localization by the model.\\n• Unambiguous Task Instructions (without direct solutions):\\nProvide clear instructions about the exact scope of changes allowed\\n(e.g., minimal edits, specific functions to modify), without giving\\ndirect code solutions.\\n• Specific Semantic Hints:\\nInclude targeted hints that clarify the intended semantic behavior\\nof the code, guiding the LLM’s reasoning.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 12}, page_content='• Exact Output Formatting (JSON Templates):\\nClearly specify the exact JSON template or diff format\\nrequired for compatibility with automated evaluation tools.\\nThis structured prompt engineering strategy significantly improved LLM accuracy\\nand reduced misalignment in generated patches.\\n3.2\\nPrompt Compression Strategies\\nTo optimize computational efficiency and reduce API costs, we developed a two-stage\\nprompt compression pipeline. Small, less computationally expensive LLMs first prepro-\\ncessed verbose prompts, followed by larger LLMs generating final repair patches. Our\\ncompression methods included:\\n1. Semantic Relevance Filtering: Leveraging smaller models to retain only content\\nsemantically relevant to bug localization and fixes, discarding irrelevant explana-\\ntions or general background context.\\n2. Heuristic-based Removal of Boilerplate: Using domain-specific heuristics to\\nsystematically eliminate standard boilerplate such as license headers, repetitive de-\\nscriptions, and non-critical comments, without altering essential control-flow struc-\\ntures or critical instructions.\\nThis two-pronged compression strategy effectively preserved crucial context and in-\\nstructions while significantly reducing token usage and computational resources.\\n3.3\\nMinimal Information Principle\\nThe theoretical foundation guiding our approach was the minimal information principle,\\nemphasizing that prompts should contain the minimal amount of information necessary\\nto accomplish the given task. Drawing on prior empirical evidence such as LLMLingua [4]\\nand CodePromptZip [5], we systematically distinguished between essential information\\ncritical for accurate patches and redundant context that could be safely removed. Ad-\\nherence to this principle significantly improved the LLM’s efficiency and accuracy, as\\nevidenced by our empirical results.\\nIn the next chapter we detail the pipeline implementation, costs, and practical pitfalls\\nbefore turning to empirical evaluation.\\n3.3.1\\nPrompt-Compression Template and Sentinel Design\\nDesign rationale.\\nOur early ablation showed that small LLMs frequently delete the\\nfirst or last block of a prompt, treating them as boiler-plate [16]. Likewise, removing\\na rare emoji from the task line caused a 21 p.p. drop in compression-success rate (see\\n§5.2). These observations align with recent studies that (1) rare tokens such as emojis\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 13}, page_content='form a high-salience attention channel [17]1, and (2) arbitrary re-ordering of prompt\\nblocks degrades model performance [19]. Hence we lock the order (Rule 0) and prefix\\ncritical blocks with single-token emojis (, ).\\nTemplate skeleton.\\nThis shows the condensed template.\\n*** Rule 0\\n(ordering)\\nkeep blocks <1><9> EXACTLY in this order.\\n*** Deleting or re-ordering a block will invalidate the prompt.\\n<1>\\n[BUG_EXPLANATION]\\n<2>\\n[TASK]\\nYour task: [TASK_DESCRIPTION]\\n<3>\\nRequirements (verbatim, do not edit)\\nFile: [FILEPATH]\\nUse a valid diff --git with correct hunk header (e.g. @@ -[LINE_NUMBERS] @@)\\nModify at least one line meaningfully (no identical +/-)\\nMust apply cleanly and pass all tests\\n<4>\\n[JSON]\\nReturn **only** this JSON object (no explanation):\\n{\\n\"instance_id\": \"[REPOSITORY_ID]\",\\n\"model_patch\": \"<your unified diff here>\",\\n\"model_name_or_path\": \"compress_result\"\\n}\\n<5>\\nBuggy function (full, unaltered)\\n‘‘‘python\\ndef function_name(parameters):\\n\"\"\"\\nDocstring must be preserved exactly.\\n\"\"\"\\n# Comments must remain unchanged\\n# Original buggy implementation\\n<6> Problematic line (highlighted exactly)\\npython\\n[PROBLEMATIC_LINE]\\n<7> Repeat blocks <5><6> for additional buggy functions if needed\\n<8> DO NOT generate any diff or patch. Your task is only to compress the instructions,\\n,→\\nnot to solve the bug.\\n<9> Preserve original visual structure (bullet layout, code fences, separators). End\\n,→of template.\\nListing 3.1: Compression template without Emoji or Unicode circled numbers\\nHuman effort boundaries.\\nHuman input is restricted to (1) locating the buggy line(s)\\nand (2) providing a one-sentence [BUG EXPLANATION].\\nAll subsequent steps—prompt\\ncompression, large-model patch generation, and SWE-bench evaluation—are fully auto-\\nmated.\\n1See also Google Cloud’s “emoji-jailbreak” post for practical evidence of their special status [18].\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 14}, page_content='Chapter 4\\nImplementation Details and Challenges\\nChapter 5 then presents the empirical evaluation of the approach.\\nIn this chapter, we discuss the key practical difficulties encountered when integrating our\\nprompt-compression and patch-generation pipeline, and outline how we mitigated them.\\n4.1\\nComprehensive Cost Analysis with Full Iterative\\nCycle\\nTo accurately reflect the complete cost associated with each issue instance in our debug-\\nging pipeline, we include the full iterative cycle: prompt compression, patch generation,\\nand SWE-bench evaluation. Each step involved repeated iterations to achieve optimal\\nresults.\\nPrompt Compression (Qwen2.5-14B):\\nAs previously described, each compression\\niteration involved:\\n• Input tokens per iteration: 5000 tokens\\n• Output tokens per iteration: 500 tokens\\n• Samples per iteration: 10\\n• Number of iterations: 22.5 (average)\\nCalculating again, we have:\\nInput token cost:\\n5000 × 10 × 22.5 ×\\n$0.2625\\n1, 000, 000 = $0.2953\\nOutput token cost:\\n500 × 10 × 22.5 ×\\n$0.525\\n1, 000, 000 = $0.0591\\nTotal prompt compression cost per instance:\\n$0.2953 + $0.0591 = $0.3544\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 15}, page_content='Patch Generation with Compressed Prompt (Qwen2.5-72B):\\nAfter obtaining\\na compressed prompt, we conducted approximately 22.5 iterations of patch generation to\\nvalidate prompt efficacy:\\n• Input tokens per iteration (compressed prompt): 500 tokens\\n• Output tokens per iteration (patch): 200 tokens\\n• Samples per iteration: 10\\n• Number of iterations: 22.5 (average)\\nThus, the patch generation cost is:\\nInput token cost:\\n500 × 10 × 22.5 ×\\n$0.40\\n1, 000, 000 = $0.045\\nOutput token cost:\\n200 × 10 × 22.5 ×\\n$0.75\\n1, 000, 000 = $0.03375\\nTotal patch generation cost per instance:\\n$0.045 + $0.03375 = $0.07875\\nSWE-bench Evaluation Cost:\\nEach generated patch was tested using SWE-bench\\nevaluation, averaging 45 seconds per test.\\nGiven 22.5 iterations and 10 samples per\\niteration:\\n22.5 iterations×10 samples/iteration×45 seconds/sample = 10125 seconds ≈2.81 hours\\nWhile SWE-bench evaluations do not directly incur API costs, this time represents\\nsignificant human and computational resource expenditure.\\nCombined API Cost Summary:\\nSummarizing API costs for both prompt compres-\\nsion and patch generation phases:\\n• Prompt Compression Cost: $0.3544\\n• Patch Generation Cost: $0.07875\\nTotal API cost per instance:\\n$0.3544 + $0.07875 = $0.43315\\nHuman Effort and Time Investment:\\nBeyond API costs, significant human effort\\nwas involved in this iterative cycle. Specifically, each instance required approximately 5\\nhours of manual experimentation and validation, including prompt compression adjust-\\nments, patch verification, and SWE-bench evaluations.\\nAfter initial trials (the first two instances), a customized prompt template was devel-\\noped, substantially improving efficiency. This refinement reduced human intervention by\\napproximately 2 hours per subsequent instance, underscoring the importance of template\\noptimization in managing overall costs.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 16}, page_content='Practical Implications and Recommendations:\\nThis comprehensive analysis em-\\nphasizes that the true cost of optimizing debugging prompts extends beyond monetary\\nexpenditure, highlighting substantial investments in human effort and computational re-\\nsources. Thus, we recommend:\\n• Systematically developing heuristic-based prompt templates to minimize trial-and-\\nerror overhead.\\n• Automating preliminary evaluations to quickly eliminate ineffective prompts before\\nresource-intensive validation.\\n• Periodically reviewing and refining prompt templates to incorporate accumulated\\ninsights, further reducing iterative experimentation and associated costs.\\nImplementing these strategies effectively balances high-quality patch generation with\\ncost and resource optimization.\\n4.1.1\\nDataset-wide Integrity Audit\\nBefore running any model-generated patches we performed a sanity check across the\\nSWE-bench Lite snapshot (24 repositories, 300 test issues) to confirm that the official\\nground-truth diffs still apply and compile on a clean checkout of each project.\\nProcedure.\\nListing 4.1 shows the Python script we used to retrieve the dataset via the\\nHugging Face API, filter to .py diffs, and emit a single gold results.json array. For\\nevery entry we then:\\n[label=()]checkout the commit hash recorded in metadata.json; apply the official\\npatch with git apply --check; run the repository-specific test command stored\\nin the dataset (pytest, tox, or Makefile target).\\nAggregate result.\\nTable 4.1 shows that 63.7 % of ground-truth patches now fail\\nbefore tests even run, mainly due to context drift.\\nTable 4.1: Replay of SWE-bench Lite ground-truth patches (300 issues)\\nOutcome\\nCount\\nShare\\nResolved – tests pass\\n107\\n35.7 %\\nUnresolved – tests fail\\n2\\n0.7 %\\nError – patch rejected\\n191\\n63.7 %\\nTotal\\n300\\n100 %\\nFailure taxonomy (40-case sample).\\n1.2.3.• Context/line drift ( 70 %): hunk headers no longer match, producing patch\\nunexpectedly ends in middle of line.\\n• API or test evolution ( 30 %): patch applies, but updated unit tests import\\nsymbols that have moved or changed signature.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 17}, page_content='Impact on evaluation.\\nThe 191 error instances are labelled Unknown rather than\\nUnknown in all pass-rate calculations in Chapter 5. This finding reinforces the need for\\nroutine benchmark refreshes; see Section 7.4.\\n\"\"\"\\ncollect_patches_array.py\\nBuilds gold_results.json ::\\n[\\n{\"instance_id\": \"...\",\\n\"model_patch\": \"...\",\\n\"model_name_or_path\": \"gold\"},\\n...\\n]\\n\"\"\"\\nimport json, re\\nfrom pathlib import Path\\nfrom datasets import load_dataset\\nHF_DATASET = \"princeton-nlp/SWE-bench_Lite\"\\nSPLIT\\n= \"test\"\\nOUT_FILE\\n= Path(\"gold_results.json\")\\nTARGET_REPO\\n= None\\n# e.g. \"astropy/astropy\" or None (all)\\nONLY_PY_FILE = True\\nDIFF_RE = re.compile(r\"^diff␣--git␣a/(.+?)␣b/\\\\\\\\1\", re.MULTILINE)\\ndef modified_files(diff): return DIFF_RE.findall(diff)\\nds = load_dataset(HF_DATASET, split=SPLIT)\\nresults = []\\nfor row in ds:\\nif TARGET_REPO and row[\"repo\"] != TARGET_REPO:\\ncontinue\\nif ONLY_PY_FILE and not any(f.endswith(\".py\")\\nfor f in modified_files(row[\"patch\"])):\\ncontinue\\nresults.append({\\n\"instance_id\": row[\"instance_id\"],\\n\"model_patch\": row[\"patch\"],\\n\"model_name_or_path\": \"gold\"\\n})\\nOUT_FILE.write_text(json.dumps(results, indent=2, ensure_ascii=False))\\nprint(f\"[INFO]␣wrote␣{len(results)}␣entries␣->␣{OUT_FILE}\")\\nListing 4.1: Collecting all SWE-bench Lite ground-truth patches\\n4.2\\nBlack-box Model Behavior\\nA significant challenge in our debugging workflow is the black-box nature of large language\\nmodels (LLMs). Unlike traditional software components, LLMs provide no introspection\\nhooks (e.g. stack traces or attention maps) that tell us which prompt elements determine\\nthe final patch. Safely compressing prompts without deleting crucial context therefore\\nrequired extensive experimentation.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 18}, page_content='Challenges Due to Lack of Introspection\\n• Unclear importance of prompt elements.\\nDocstrings, comments, or even\\nwhitespace sometimes changed the model’s output in unpredictable ways, making\\nit hard to decide what could be pruned.\\n• Model-specific sensitivity. Smaller models (qwen2.5-14b-instruct) ignored\\ndo-not-snip directives, whereas the 72B model respected them, forcing us to tailor\\nprompts per model size.\\n• Opaque formatting effects.\\nMarkdown headings (#) suppressed instruction\\nsalience (see Section 5.3); such effects are invisible until a patch fails.\\n4.2.1\\nMotivating Example: Silent Patch Failure\\nListing 5.1 shows how removing a single isinstance branch caused the LLM to halluci-\\nnate a patch that overwrote the array with 1s instead of copying right. No error message\\nwas produced; the only symptom was a failing unit test. This silent failure motivated the\\nablation study described next.\\n4.2.2\\nEmpirical Evidence and Impact on Workflow Efficiency\\nIssue astropy-12907 illustrates the broader pattern: elements we first deemed “super-\\nfluous” (e.g. a seemingly redundant conditional at line 245) turned out to be essential for\\na correct patch. Discovering such dependencies required repeated prompt-ablation runs,\\nincreasing wall-clock time and token cost. Overall, black-box sensitivity added roughly\\ntwo extra prompt iterations per issue, or 30\\n4.2.3\\nMitigation Strategies\\n• Incremental ablation. We removed prompt elements one at a time and reran the\\nmodel; if the patch failed, the element was marked “critical”.\\n• Heuristic prompt templates.\\nInsights from early trials were distilled into a\\ntemplate that explicitly preserves control-flow and bug-description lines, cutting\\nlater experimentation time by 40\\n• Cross-model verification.\\nWe compared outputs from 14B and 72B models:\\ndisagreements signalled a high-risk prompt, triggering an extra review pass.\\nDespite these measures, the black-box nature of LLMs remains a core limitation,\\nnecessitating continuous empirical validation in prompt-driven debugging workflows.\\n4.3\\nStrict Formatting and Output Constraints\\nStrict formatting constraints significantly impacted our workflow. Even minor deviations\\nfrom required diff or JSON formats could halt automated evaluation entirely.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 19}, page_content='4.3.1\\nChallenges Encountered\\nWe encountered several practical issues due to strict formatting constraints:\\n• Diff Formatting Issues: Missing or incorrectly specified diff headers (e.g., @@\\n-245,7 +245,7 @@) caused parser errors in SWE-bench. File prefixes (e.g., a/, b/)\\nwere also sometimes required to avoid patching failures.\\n• Whitespace and Trailing Character Errors: Trailing whitespace or line end-\\nings (e.g., \"\\\\n \" or extra newlines at EOF) often led to patch application errors\\nsuch as:\\nunexpected end of file in patch\\n4.3.2\\nConcrete Examples\\nAn incorrectly formatted diff missing file prefixes:\\n@@ -245,7 +245,7 @@ def _cstack(left, right):\\nelse:\\ncright = np.zeros((noutp, right.shape[1]))\\n-\\ncright[-right.shape[0]:, -right.shape[1]:] = 1\\n+\\ncright[-right.shape[0]:, :right.shape[1]] = right\\nListing 4.2: Incorrectly formatted diff patch\\nAn example of trailing whitespace that broke the parser:\\n\"model_patch\": \"diff --git a/file.py b/file.py\\\\n@@ -10,7 +10,7 @@ def function\\n,→():\\\\n-\\nreturn x\\\\n+\\nreturn y \\\\n\"\\nListing 4.3: Trailing whitespace causing parse error\\nThis led to errors like:\\nunexpected end of file in patch\\n4.3.3\\nMitigation Strategies\\nTo address these formatting issues, we implemented several targeted solutions:\\n• JSON Schema Validation: We used the jsonschema Python library to enforce\\nstructural correctness for JSON files, reducing the chance of malformed outputs.\\n• Explicit Prompt Constraints: Our prompt templates included precise unified\\ndiff requirements, including correct file paths and hunk headers:\\nRequirements:\\n- File: astropy/nddata/mixins/ndarithmetic.py\\n- Use a valid diff --git with correct hunk header\\n(e.g., @@ -520,10 +520,10 @@)\\n- Modify at least one line meaningfully (no identical +/-)\\n- Must apply cleanly and pass all tests\\nListing 4.4: Explicit diff hunk header requirement\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 20}, page_content='• Whitespace and Trailing Character Removal: We developed a custom utility\\n(fix patch trailing space.py) to scan and clean trailing whitespace in model-generated\\npatches. This eliminated nearly all “unexpected end of file” errors during SWE-\\nbench evaluation.\\n4.3.4\\nBenchmark Data Version Mismatch\\nIn certain cases, even patches that exactly matched SWE-bench’s official ground truth\\nfailed to apply. This suggests that the benchmark’s file snapshots and local cloned repos-\\nitories can occasionally diverge.\\nExample: Issue astropy-6938\\nWe submitted the following patch—verbatim from the\\nSWE-bench dataset:\\n{\\n\"instance_id\": \"astropy__astropy-6938\",\\n\"model_patch\": \"diff --git a/astropy/io/fits/fitsrec.py b/astropy/io/fits/fitsrec.py\\n,→\\\\n@@ -1261,7 +1261,7 @@ def _scale_back_ascii(self, col_idx, input_field,\\n,→output_field):\\\\n\\nif ’D’ in format:\\\\n\\n# Replace exponent\\n,→separator in floating point numbers\\\\n-\\noutput_field.replace(\\n,→encode_ascii(’E’), encode_ascii(’D’))\\\\n+\\noutput_field[:] =\\n,→output_field.replace(b’E’, b’D’)\\\\n\",\\n\"model_name_or_path\": \"compress_result\"\\n}\\nListing 4.5: Generated patch payload matching official dataset\\nThe evaluation returned:\\npatching file astropy/io/fits/fitsrec.py\\npatch unexpectedly ends in middle of line\\npatch: **** unexpected end of file in patch\\nDiagnosis\\nThe benchmark likely uses slightly different line endings, context lines, or\\nspacing. These inconsistencies—though semantically irrelevant—break the patching pro-\\ncess.\\nSuggested Solutions\\n• Dataset Synchronization: Ensure local files match SWE-bench snapshot ver-\\nsions. Re-cloning or verifying file hashes can help.\\n• Patch Fuzzing: Use git apply --reject --whitespace=fix or GNU patch\\n--fuzz=2 to allow slight mismatches in context lines.\\n• Pre-evaluation Diff Check: Automate a comparison between local buggy files\\nand dataset files before running final evaluation.\\n4.3.5\\nSummary\\nFormat fragility was a consistent source of patch failure. By combining strict schema\\nvalidation, template constraints, cleanup scripts, and better benchmark alignment, we\\nwere able to reduce evaluation failures by more than 80% in later experiments.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 21}, page_content='4.4\\nIterative vs. One-shot Debugging Cost Analysis\\nTo rigorously analyze debugging cost implications, we performed detailed comparisons be-\\ntween iterative and one-shot methods. Basic experimental parameters (e.g., token counts,\\nsampling rates) were described earlier in Section 5.1; here we provide a comprehensive\\ncost analysis.\\n4.4.1\\nDetailed Cost Analysis of Iterative Debugging (GPT-4o)\\nFor issue astropy-12907, the iterative debugging approach required around 20 rounds\\nof GPT-4o API interactions. At an average token usage of approximately 4000 tokens\\nper interaction (input + output), each interaction costs approximately $0.02 (assuming\\n$5 per million tokens):\\n4000\\n1, 000, 000 × 5 = $0.02\\nThe total iterative debugging cost therefore averages around:\\n20 requests × $0.02 = $0.40\\nConsidering additional overheads (re-establishing context, detailed explanations), real-\\nworld costs typically reached approximately $0.50–$0.70 per issue instance.\\n4.4.2\\nDetailed Cost Analysis of One-shot Debugging (Qwen2.5)\\nThe one-shot debugging strategy, optimized through prompt compression (Qwen2.5-14B)\\nand patch generation (Qwen2.5-72B), incurred the following costs:\\n• Prompt Compression Cost: Approximately $0.07 per instance (25 iterations ×\\n10 samples × 550 tokens each).\\n• Patch Generation Cost: Approximately $0.25 per instance (25 iterations × 10\\nsamples × 700 tokens each).\\nThus, the combined API cost is around:\\n$0.07 + $0.25 = $0.32\\nHowever, substantial human time (approx. 5 hours per issue) was required to craft,\\nrefine, and validate effective one-shot prompts. Although automated techniques (template\\ncreation, scripts) reduced this to roughly 3 hours per subsequent issue, the human labor\\ncost remained significant.\\n4.4.3\\nComparative Cost-benefit Analysis\\nThe iterative approach, despite higher API costs, generally required less upfront human\\ninvestment. Conversely, the one-shot method, while economically cheaper in API terms,\\ninvolved extensive initial human tuning efforts. Practitioners should carefully weigh the\\ncost of human labor against computational efficiency when choosing between these ap-\\nproaches.\\nHaving addressed these implementation challenges, we next evaluate the approach\\nempirically in Chapter 5.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 22}, page_content='Table 4.2: Detailed cost comparison of iterative vs. one-shot debugging\\nMethod\\nAPI Cost\\nHuman Effort\\nNotes\\nIterative (GPT-4o)\\n$0.50–$0.70\\nModerate\\nRequires ∼20 API calls; min-\\nimal prompt engineering ef-\\nfort.\\nOne-shot (Qwen2.5)\\n$0.32\\nHigh\\nLower API cost but involves\\nsignificant initial prompt de-\\nsign (∼3–5 h per issue).\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 23}, page_content='Chapter 5\\nExperimental Evaluation and Analy-\\nsis\\nIn this chapter, we present our experiments on prompt compression and code patch\\ngeneration. We focus particularly on how providing complete function code context, as\\nwell as explicit instructions, is vital for producing patches that pass validation tests.\\n5.1\\nEvaluation Configuration\\nWe ran each of the six Astropy issues through two prompt variants, sampling 10 outputs\\nat temperature 0.5. Prompt lengths:\\n- Pre-compression: 3 000–5 000 tokens - Post-compression: 500–600 tokens\\nMetrics collected:\\n• Compression-Success Rate:\\n# prompts that satisfy all format rules\\n10 candidates\\n× 100%.\\n• Patch-Pass Rate:\\n# generated patches that pass SWE-bench\\n10 candidates\\n× 100%.\\n• Token Reduction: Average difference in token count between the original prompt\\nand its compressed form, reported only for prompts that meet the compression-success\\ncriterion.\\n5.2\\nCase Studies\\n5.2.1\\nImpact of Function Context Completeness on Patch Ac-\\ncuracy (Issue astropy-12907)\\nIn this experiment, we investigated how the completeness of the provided function context\\nimpacts the accuracy and correctness of the generated patches. We specifically analyzed\\nissue astropy-12907, which addresses incorrect separability matrix computations for\\nnested CompoundModels.\\nWe compared two compressed prompts that were identical in terms of their textual\\ninstructions and semantic explanations. The only difference was in the completeness of\\nthe provided function context.\\nIncomplete Function Context (First Prompt)\\nThe first compressed prompt provided only a partial snippet, removing essential condi-\\ntional logic, type checks, and other structural elements:\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 24}, page_content='def _cstack(left, right):\\n...\\nelse:\\ncright = np.zeros((noutp, right.shape[1]))\\ncright[-right.shape[0]:, -right.shape[1]:] = 1\\nreturn np.hstack([cleft, cright])\\n...\\nListing 5.1: Incomplete context provided to LLM\\nDetails Removed:\\n• Complete conditional checks (isinstance(left, Model), isinstance(right, Model)).\\n• Detailed array initialization logic and parameter handling.\\n• Function documentation and explicit parameter annotations.\\nResulting Incorrect Patch:\\n@@ -245,7 +245,7 @@ def _cstack(left, right):\\nelse:\\ncright = np.zeros((noutp, right.shape[1]))\\n-\\ncright[-right.shape[0]:, -right.shape[1]:] = 1\\n+\\ncright[-right.shape[0]:, :right.shape[1]] = 1\\nreturn np.hstack([cleft, cright])\\nListing 5.2: Incorrect patch from incomplete context\\nThis patch failed because it incorrectly maintained the assignment of the constant\\n1 instead of assigning the input array right, demonstrating a misunderstanding due to\\nmissing conditional context.\\nComplete Function Context (Second Prompt)\\nThe second prompt included comprehensive details of the function:\\ndef _cstack(left, right):\\n\"\"\"\\nFunction corresponding to ’&’ operation.\\nParameters\\n----------\\nleft, right : ‘astropy.modeling.Model‘ or ndarray\\nIf input is an array, it is the output of ‘coord_matrix‘.\\nReturns\\n-------\\nresult : ndarray\\nResult from this operation.\\n\"\"\"\\nnoutp = _compute_n_outputs(left, right)\\nif isinstance(left, Model):\\ncleft = _coord_matrix(left, ’left’, noutp)\\nelse:\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 25}, page_content='cleft = np.zeros((noutp, left.shape[1]))\\ncleft[: left.shape[0], : left.shape[1]] = left\\nif isinstance(right, Model):\\ncright = _coord_matrix(right, ’right’, noutp)\\nelse:\\ncright = np.zeros((noutp, right.shape[1]))\\ncright[-right.shape[0]:, -right.shape[1]:] = 1\\nreturn np.hstack([cleft, cright])\\nListing 5.3: Complete context provided to LLM\\nDetails Retained:\\n• Full conditional logic and type-checking (isinstance(..., Model)).\\n• Complete array initialization and indexing logic.\\n• Complete function docstring, clearly outlining function behavior.\\nResulting Correct Patch:\\n@@ -256,7 +256,7 @@ def _cstack(left, right):\\ncright = _coord_matrix(right, ’right’, noutp)\\nelse:\\ncright = np.zeros((noutp, right.shape[1]))\\n-\\ncright[-right.shape[0]:, -right.shape[1]:] = 1\\n+\\ncright[-right.shape[0]:, -right.shape[1]:] = right\\nreturn np.hstack([cleft, cright])\\nListing 5.4: Correct patch from complete context\\nHere, the model accurately identified and corrected the semantic error, directly as-\\nsigning the array right, thereby preserving the input’s structure.\\nClarification of Code Context Setup\\nIt is important to emphasize that the textual instruction portions (including Problem\\nStatement, Task, Requirements, and Output Format) of these two compressed\\nprompts were identical. The only distinguishing factor was the completeness of the func-\\ntion code snippet provided to the model. Thus, differences observed in patch accuracy\\ncan be directly attributed to the extent of function code context provided.\\nKey Observations and Recommendations\\nThis experiment underscores the critical importance of providing complete structural\\ncontext in debugging prompts to facilitate accurate semantic reasoning and correct patch\\ngeneration by LLMs. Removing structural or logical details significantly increases the\\nrisk of incorrect or superficially valid patches that fail to address the underlying issue.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 26}, page_content='5.2.2\\nSuccessful vs. Failing Compression in ndarithmetic.py (Is-\\nsue astropy-14995)\\nWe now examine issue astropy-14995, related to incorrect mask propagation in the\\narithmetic mask method. Unlike issue astropy-12907, where differences in patches\\nwere primarily due to the completeness of the function code context provided, here we\\nexplore how subtle differences in semantic instruction clarity and emphasis impact patch\\ncorrectness, given that both prompts provided the complete function code.\\nSuccessful Semantic Compression\\nIn the successfully compressed prompt, critical\\nsemantic context clearly highlighted the exact problematic line and explicitly emphasized\\nthe nature of the semantic error:\\n**Problem Statement**:\\nIn v5.3, NDDataRef mask propagation fails when one operand lacks a mask, specifically\\n,→with handle_mask=np.bitwise_or, causing a TypeError.\\n**Task**: Apply minimal fix (12 lines) for mask propagation when operand has no mask.\\n,→Ensure all tests in test_nddata_bitmask_arithmetic() pass.\\n### Code Section to Preserve:\\ndef _arithmetic_mask(self, operation, operand, handle_mask, axis=None, **kwds):\\n# ... (complete function provided in prompt)\\n### Specific Bug Description:\\nThe condition ‘elif operand is None:‘ incorrectly checks operand instead of operand.\\n,→mask, causing bitwise TypeError.\\n### Requirements:\\n- File: astropy/nddata/mixins/ndarithmetic.py\\n- Use valid diff --git\\n- Modify meaningfully\\n- Must pass all tests\\n**Return Format**\\n{\\n\"instance_id\": \"astropy__astropy-14995\",\\n\"model_patch\": \"<your unified diff here>\",\\n\"model_name_or_path\": \"compress_result\"\\n}\\nListing 5.5: Successful compressed semantic prompt\\nThis prompt succeeded because:\\n• It explicitly pinpointed the problematic line (elif operand is None:).\\n• It clearly and explicitly clarified the semantic nature of the error (”should check if\\noperand.mask is None instead”).\\n• It provided precise and explicit test-passing criteria.\\nFailing Semantic Compression\\nThe unsuccessful compressed prompt, though struc-\\nturally similar, significantly omitted the initial explicit semantic explanation from the\\noriginal detailed instructions, relying instead on generic and ambiguous phrasing:\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 27}, page_content='**Compressed Instructions**\\n### Requirements:\\n- File: astropy/nddata/mixins/ndarithmetic.py\\n- Valid diff --git\\n- Modify meaningfully, apply cleanly, pass tests\\n### Uncompressed Buggy Function:\\ndef _arithmetic_mask(self, operation, operand, handle_mask, axis=None, **kwds):\\n# ... (complete function provided in prompt)\\n### Specific Bug Description:\\n- Problematic Line: ‘elif operand is None:‘\\n- Explanation: \"The condition ‘elif operand is None:‘ incorrectly checks operand\\n,→instead of operand.mask, causing TypeError.\"\\n- Task: \"Minimal fix (12 lines) to pass all tests.\"\\n### Wanted Format:{\\n\"instance_id\": \"astropy__astropy-14995\",\\n\"model_patch\": \"<your unified diff here>\",\\n\"model_name_or_path\": \"compress_result\"}\\nListing 5.6: Failing compressed semantic prompt\\nThis prompt failed due to:\\n• Lack of explicit and clear semantic context (”should check operand.mask instead”).\\n• Reliance on generic phrasing, reducing the emphasis on the logic-critical condition.\\n• Over-generalization, causing semantic ambiguity for the LLM.\\nClarification of Semantic Instruction Setup\\nImportantly, unlike the previous ex-\\nperiment (astropy-12907), both prompts provided the complete function code.\\nThe\\nprimary difference lay in the explicitness of the semantic instruction—how clearly and\\nstrongly the prompt emphasized the critical logical error.\\nComparison and Analysis\\nThis comparison highlights the critical role of semantic\\nclarity and explicitness in instruction compression. Effective prompt compression requires\\nretaining explicit semantic clarifications about the nature of the bug. Even subtle reduc-\\ntions in semantic precision or clarity significantly degrade the model’s ability to correctly\\naddress the targeted logical errors.\\nHence, while structural completeness of the code is essential (as seen in issue astropy-12907),\\nsemantic precision is equally vital. Ambiguous or overly generalized task descriptions di-\\nminish model performance, emphasizing the necessity for explicitly retaining semantic\\ndetails to guide correct patch generation.\\n5.2.3\\nInstruction Ignoring by Small Model (Issue 14995)\\nAlthough we explicitly instructed the smaller model qwen2.5-14b-instruct not to re-\\nmove or compress any parts of the provided function code, the model still discarded\\ncritical components, including significant portions of the docstring and surrounding func-\\ntion context. This behavior aligns with recent findings indicating that small LLMs ( 14B\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 28}, page_content='parameters) frequently exhibit difficulty maintaining fine-grained instruction adherence\\nand format fidelity, particularly when confronted with complex or multi-part directives,\\ncausing unintended truncation or omission of essential input content.\\n]Observed Snipping of Docstring\\ndef _arithmetic_mask(self, operation, operand, handle_mask, axis=None, **kwds):\\n\"\"\"\\nCalculate the resulting mask.\\n...\\n\"\"\"\\nif (\\nself.mask is None and operand is not None and operand.mask is None\\n) or handle_mask is None:\\nreturn None\\nelif self.mask is None and operand is not None:\\nreturn deepcopy(operand.mask)\\nelif operand is None:\\nreturn deepcopy(self.mask)\\nelse:\\nreturn handle_mask(self.mask, operand.mask, **kwds)\\nListing 5.7: Small model’s unintended snip of docstring (issue 14995)\\nBecause the docstring and intermediate comments were removed, the subsequent\\npatch invocation failed with:\\nHunk #1 succeeded at 521 with fuzz 2 (offset 1 line).\\nExtreme Snipping to Buggy Line Only\\nIn some runs, the model pared the prompt\\ndown to just the buggy lines:\\n# Buggy Code Section:\\nelif operand is None:\\nreturn deepcopy(self.mask)\\nListing 5.8: Further extreme snip to only the buggy line\\nThis left the model without any context of the surrounding control flow, guaranteeing\\nthat the generated patch would not apply cleanly.\\nAnalysis\\nSmall models like qwen2.5-14b-instruct often prioritize brevity over in-\\nstruction fidelity when faced with long or complex prompts . As a result, even explicit\\n“do not snip” instructions are under-weighted, causing loss of essential context and mis-\\naligned hunks. This behavior underscores the need for either stronger prompt-tuning\\ntechniques or hybrid workflows that reserve context-preservation duties for larger, more\\ninstruction-aligned models.\\n5.3\\nEffect of Markdown Formatting on Instruction\\nSensitivity\\nIn the process of refining prompt designs, we observed a surprising and consistent behav-\\nior: markdown heading symbols (i.e., #) tend to suppress model attention to the actual\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 29}, page_content='task content. This phenomenon was identified while working on issue astropy-6938,\\nwhich involves ensuring correct in-place replacement behavior in NumPy arrays.\\nPrompt Using Markdown Headings (#)\\nInitially, the instructions were written\\nusing markdown-style headings:\\n#### Task Description:\\n#\\nYour task: Apply a minimal fix (1 line) to make the character replacement work\\n,→correctly.\\n# The patch must ensure proper conversion of exponent notation from ’E’ to ’D’ format\\n,→in ASCII output fields.\\n# Note: The ‘replace‘ method on ’chararray’ objects does not modify the array in place\\n,→.\\n# It returns a new array, leaving the original unchanged.\\n# TODO: Ensure that output_field is actually updated in place.\\n# (Hint: this fix only requires 1 line)\\n# (Hint: think about how assignments work for NumPy array slices vs. full rebindings)\\nListing 5.9: Prompt with markdown headings\\nDespite repeated runs, the model consistently failed to generate the expected behavior\\ninvolving the slice assignment output field[:]. The output instead looked like:\\n@@ -1304,7 +1304,7 @@ def _scale_back_ascii(self, col_idx, input_field, output_field):\\nif ’D’ in format:\\n-\\noutput_field.replace(encode_ascii(’E’), encode_ascii(’D’))\\n+\\noutput_field = output_field.replace(encode_ascii(’E’), encode_ascii(’D’))\\nListing 5.10: Patch generated using heading-style prompt (unexpected behavior)\\nAlthough syntactically valid, this patch performs a re-binding rather than an in-place\\nupdate, violating the requirement described in the hints.\\nPrompt Using Bullet Points and Bold Text\\nWe then rewrote the same task using\\nlist-based formatting with bold emphasis:\\n**Task Description**:\\n- Apply a minimal fix (1 line) to make the character replacement work correctly.\\n- Ensure proper conversion of exponent notation from ’E’ to ’D’ format in ASCII output\\n,→\\nfields.\\n- Note: The ‘replace‘ method on ‘chararray‘ objects does not modify the array in place\\n,→.\\n- It returns a new array, leaving the original unchanged.\\n- Ensure that ‘output_field‘ is actually updated in place.\\n- (Hint: this fix only requires 1 line)\\n- (Hint: think about how assignments work for NumPy array slices vs. full rebindings)\\nListing 5.11: Prompt using bullet point formatting\\nThis alternative formatting resulted in significantly improved responses: in nearly all\\nruns, the model correctly utilized the expected output field[:] in the patch. A typical\\noutput was:\\n@@ -1304,7 +1304,7 @@ def _scale_back_ascii(self, col_idx, input_field, output_field):\\nif ’D’ in format:\\n-\\noutput_field.replace(encode_ascii(’E’), encode_ascii(’D’))\\n+\\noutput_field[:] = output_field.replace(encode_ascii(’E’), encode_ascii(’D\\n,→’))\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 30}, page_content='Listing 5.12: Patch generated using bullet-point prompt (matches expectations)\\nInterpretation\\nThis experiment demonstrates that LLMs like qwen2.5-72b-instruct\\ntend to treat markdown headings as high-level summary labels, not as operational in-\\nstructions. As a result, critical task directives embedded in ‘‘-prefixed lines were often\\ndown-weighted or ignored entirely.\\nConversely, formatting the same content as a bullet list, with strong verb cues and\\nsemantic highlighting (e.g., Hint: and emphasis on [:]) helped the model focus on the\\ncorrect modification intent.\\nRecommendation\\nWe recommend avoiding markdown headings (#) for detailed, ac-\\ntionable instructions in prompts. Use list-based structures and visually separable format-\\nting (such as bold text, dash lists, or boxed hints) to guide model attention and improve\\nexecution reliability on subtle implementation tasks.\\n5.4\\nUnexpected Language Output from Qwen2.5-14B-\\nInstruct\\nDuring our prompt compression experiments for issue astropy-6938, we encountered\\na non-deterministic and unexpected behavior when using qwen2.5-14b-instruct. Al-\\nthough the prompt was written entirely in English, the compressed instructions returned\\nby the model were partially rendered in Russian.\\nThis occurred without any multilingual hints in the input, and the affected output\\nsegment contained key instructions such as patch format, diff requirements, and JSON\\nformat—all written in Russian.\\nFigure 5.1 shows the full context, and an excerpt is\\nreproduced below:\\nFigure 5.1: Excerpt of unexpected Russian output from Qwen2.5-14B-Instruct.\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 31}, page_content='Analysis and Implications\\nThis anomaly is particularly interesting because it oc-\\ncurred in a **smaller multilingual model**, not in Qwen’s 72B variant. It highlights the\\nfollowing risks when using smaller instruction-tuned LLMs:\\n• Unstable Language Mode Switching: The model may unpredictably respond\\nin a non-target language, especially in templated or structured compression tasks.\\n• Lack of Language Adherence: Even in clearly English-only contexts, smaller\\nmodels may default to a language seen during pretraining, possibly triggered by\\nlearned markdown or token patterns.\\n• Reliability Concerns in Compression Pipelines: Such responses break for-\\nmatting expectations, confuse downstream parsers, and reduce the repeatability of\\nresults.\\nMitigation Strategy\\nTo prevent this kind of drift, we recommend always appending\\nan explicit language constraint to compression instructions, such as:\\nAll content, instructions, and output must be written in English.\\nAdditionally, this behavior underscores the potential benefits of larger models in\\ncompression-critical tasks where fidelity, consistency, and deterministic formatting are\\nessential.\\n5.5\\nEnvironment Setup and Reproducibility\\nThis appendix provides an end-to-end recipe—clone, install, extract, compress, patch,\\nand evaluate—that **anyone** can run on a fresh Linux or macOS machine with a valid\\nLLM API key. Each numbered step corresponds to a bullet in Figure ?? of the main\\npaper.\\nAll artefacts produced along the way (prompts, patches, logs) are versioned in ‘solutions/¡instanceid >\\n/‘sopartialrunscanberesumedwithoutmanualcleanup.\\n5.5.1\\nDirectory layout\\nListing 5.13 shows the canonical project tree after you have generated prompts for two\\nAstropy and three Django issues. The layout is not enforced by our scripts but strongly\\nrecommended so that helper paths (and this report) stay stable across hosts.\\ncode-aware-prompt-compression/\\n-- solutions/\\n# compressed prompts + patch outputs\\n|\\n-- astropy__astropy-6938/\\n|\\n|\\n-- prompt_template.md\\n# hand-crafted starting prompt\\n|\\n|\\n-- prompt_comp_small.md\\n# step 3 output (small model)\\n|\\n|\\n-- patch_large.json\\n# step 4 output (large model)\\n|\\n|\\n-- llm_play.log\\n# generation log + token counts\\n|\\n-- astropy__astropy-12907/\\n|\\n-- ...\\n|\\n-- buggy_code_files/\\n# step 2 outputs (raw *.py)\\n|\\n-- astropy__astropy-6938_buggy.py\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 32}, page_content='|\\n-- django__django-10914_buggy.py\\n|\\n-- raw_json_prompts/\\n# step 2 outputs (JSON)\\n|\\n-- buggy_astropy__astropy-6938.json\\n|\\n-- buggy_django__django-10914.json\\n|\\n-- scripts/\\n# pipeline entry points\\n|\\n-- extract_buggy_files.py\\n|\\n-- extract_buggy_json.py\\n|\\n-- inject_buggy_function.py\\n|\\n-- patch_cleaner.py\\n|\\n-- compression_instruction.md\\n# prepend to every raw JSON\\n-- README.md\\n# markdown quick-start\\n-- requirements.txt\\nListing 5.13: Project directory layout\\n5.5.2\\nStep-by-step reproduction\\nNotation Paths that need to be changed by the reader are wrapped in ‘< angle brack-\\nets>‘.\\n1. Clone\\ninstall dependencies\\ngit clone https://github.com/your-username/code-aware-prompt-compression.git\\ncd code-aware-prompt-compression\\npip install -r requirements.txt\\n# optional, for SWE-bench evaluator CLIs\\npip install swebench\\n# configure llm-play once\\nllm-play --add-provider\\n# sets OPENAI / Anthropic keys in ~/.llm_play.yaml\\n2. (Optional but recommended) clone target repositories\\nmkdir -p ~/repos\\n# clone only the projects you intend to evaluate\\ngit clone https://github.com/astropy/astropy.git\\n~/repos/astropy\\ngit clone https://github.com/django/django.git\\n~/repos/django\\nThese local Git mirrors let the injection scripts retrieve the *exact* buggy commit\\nvia git show. If you only run the compression baseline you can skip this step and\\ntreat the JSON files as plain text.\\n3. Extract buggy code & metadata (step 2 in Fig. ??)\\npython scripts/extract_buggy_files.py\\n--repo astropy/astropy --local-repo ~/\\n,→repos/astropy\\npython scripts/extract_buggy_json.py\\n--repo astropy/astropy --local-repo ~/\\n,→repos/astropy\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 33}, page_content='4. Prepare compression prompt Concatenate the static instruction with the freshly\\ncreated JSON.\\ncat compression_instruction.md raw_json_prompts/buggy_astropy__astropy-6938.json\\n,→\\n> tmp_prompt.json\\n5. Prompt compression – small model\\nllm-play \\\\\\n--prompt tmp_prompt.json \\\\\\n--model qwen1.5-14b-chat \\\\\\n--temperature 0.4 --n 6 \\\\\\n--output solutions/astropy__astropy-6938/prompt_comp_small.md\\nInspect token savings with ttok if desired.\\n6. Inject buggy code for big-model reasoning\\npython scripts/inject_buggy_function.py \\\\\\n--prompt solutions/astropy__astropy-6938/prompt_comp_small.md \\\\\\n--repos-root ~/repos -v\\n7. Patch generation – large model\\nllm-play \\\\\\n--prompt solutions/astropy__astropy-6938/prompt_comp_small.md \\\\\\n--model claude-3-opus \\\\\\n--temperature 0.2 --n 4 \\\\\\n--output solutions/astropy__astropy-6938/patch_large.json\\n8. Evaluation with SWE-bench\\npython -m swebench.harness.run_evaluation \\\\\\n--dataset_name princeton-nlp/SWE-bench_Lite \\\\\\n--predictions_path solutions/astropy__astropy-6938/patch_large.json \\\\\\n--max_workers 4 --run_id local-test\\n9. Utility scripts (optional)\\n# count tokens before / after compression\\nttok -i tmp_prompt.json\\n# lint & normalise patch (ensures newline at EOF)\\npython scripts/patch_cleaner.py solutions/.../patch_large.json\\n5.6\\nObservations\\nBased on the detailed experimental evaluation and case studies in Section 5.2, we distill\\nseveral critical observations that reveal best practices, common pitfalls, and trade-offs\\nwhen applying prompt compression for LLM-driven debugging.\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 34}, page_content='5.6.1\\nKey Elements for Successful Prompt Compression\\nThrough careful analysis of successful and unsuccessful prompt compression scenarios,\\nwe identified specific elements consistently contributing to accurate and minimal patch\\ngeneration:\\n• Comprehensive Structural Context: It is imperative to retain the entirety of\\ncritical function logic, particularly conditional checks and control-flow statements.\\nAs illustrated in Listings 5.3 and 5.1, incomplete code snippets consistently result in\\nincorrect patches. Including the entire function context ensures accurate semantic\\ninterpretation by the model.\\n• Explicit Semantic Clarification: Clear, concise, and explicitly articulated de-\\nscriptions of the bug are crucial. Phrases such as “overwrites the array with literal\\n1s instead of copying array contents” (Section 5.6.1) significantly improve the LLM’s\\nability to target the correct logical fix.\\n• Targeted and Unambiguous Instructions: Instruction clarity, coupled with\\nprecise constraints and explicitly defined goals (such as “modify at most two lines”\\nand “must pass all tests”), effectively prevents model overgeneralization and pro-\\nmotes targeted patch generation.\\n• Strict Adherence to Output Format: Including exact JSON templates and\\nenforcing format constraints ensures that generated patches are directly usable by\\ndownstream evaluation tools, minimizing human intervention and parsing errors.\\n5.6.2\\nCompression Strategy Trade-offs\\nThe empirical evaluation demonstrates a clear trade-off between brevity and complete-\\nness. While aggressive compression significantly reduces token usage (from approximately\\n3,000–5,000 tokens to 500–600 tokens), overly aggressive strategies lead to loss of critical\\ncontext, ultimately wasting computational resources on ineffective patches.\\nOur results highlight that selectively removing boilerplate content such as docstrings\\nand license headers, while retaining explicit instructions and structural logic, strikes an\\noptimal balance. This hybrid compression approach (detailed in Section ??) consistently\\noutperforms purely code-based or purely instruction-based compression.\\n5.6.3\\nModel-Specific Behaviors and Recommendations\\nInstruction Fidelity in Smaller Models\\nSmaller LLMs (e.g., qwen2.5-14b-instruct)\\nfrequently fail to adhere strictly to fine-grained compression instructions, such as explicit\\ndirectives to avoid code snipping (see Section 5.2). This phenomenon results in inad-\\nvertent truncation of critical function context, causing patch misalignment issues (e.g.,\\n“Hunk succeeded with fuzz”). As a mitigation strategy, we recommend clearly emphasiz-\\ning critical instructions at the start of the prompt or repeating them prominently, thus\\nimproving instruction adherence.\\nLanguage and Formatting Sensitivity\\nOur experiments revealed surprising model\\nbehaviors associated with prompt formatting:\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 35}, page_content='• Markdown heading syntax (#) consistently diminishes model attention to the de-\\ntailed content of instructions, causing misinterpretation of tasks or instructions\\n(refer to detailed analysis in Section 5.2).\\n• Parenthetical hints, when enclosed within parentheses, are frequently compressed or\\nomitted entirely, indicating their perceived lower semantic weight by smaller LLMs.\\nTo enhance instruction fidelity, we advise avoiding markdown headings and parenthe-\\nses for key instructional elements. Instead, utilize visually distinctive formatting such as\\nbullet points, bold emphasis, or boxed text to highlight critical semantic guidance.\\nUnpredictable Language Switching\\nSmaller multilingual LLMs such as qwen2.5-\\n14b-instruct exhibited unpredictable language mode switching, occasionally producing\\nunexpected outputs in non-target languages, notably Russian. This behavior significantly\\ndisrupts parsing processes and downstream evaluations. We suggest explicitly enforcing\\nlanguage constraints within prompts (e.g., “All instructions must be in English”) to\\nminimize this risk.\\n5.6.4\\nImplications for Real-world Debugging Workflows\\nOur observations translate directly into actionable insights for practitioners employing\\nLLM-based debugging workflows:\\n1. Comprehensive Bug Localization: Clearly identifying and explicitly stating\\nbug locations before prompt generation greatly enhances the accuracy of generated\\npatches.\\n2. Balanced Compression Strategy: Adopting a balanced approach—removing\\nonly non-essential boilerplate while retaining explicit semantic details and structural\\ncontext—maximizes model efficiency and patch reliability.\\n3. Model-Aware Prompt Design: Designing prompts with specific knowledge of\\nmodel capabilities, instruction adherence tendencies, and formatting sensitivities\\nsignificantly improves model performance and consistency.\\nThese insights not only optimize computational resource usage but also streamline\\ndebugging processes, contributing directly to more efficient software development prac-\\ntices.\\n5.6.5\\nSummary of Observations\\nIn summary, our detailed experimentation and analysis underscore several best practices\\nand considerations for effective prompt compression in debugging scenarios:\\n• Context Completeness and Semantic Precision: Essential for accurate model\\ncomprehension and patch correctness.\\n• Explicitness and Formatting Clarity: Critical for maintaining instruction fi-\\ndelity, especially in smaller LLMs.\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 36}, page_content='• Careful Management of Compression Trade-offs: Necessary to balance effi-\\nciency gains against the risk of losing essential information.\\n• Awareness of Model-specific Behaviors: Important for tailoring prompt struc-\\ntures and instructions effectively to different LLMs.\\nThese comprehensive observations form a robust foundation for future research and\\npractical application of LLM-driven debugging techniques, highlighting clear pathways\\ntoward improved model utilization and debugging efficacy.\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 37}, page_content='Chapter 6\\nDiscussion\\nThis chapter reflects on the **practical value, boundaries, and broader implications** of\\nour manual prompt–compression workflow in the era of autonomous debugging agents\\nsuch as SWE-agent. While Chapters 4–5 provided empirical and implementation-level\\ndetail, here we address four higher-level questions:\\n1. Why attempt human-guided one-shot debugging at all?\\n2. When does manual compression outperform agent-style iterative search?\\n3. Which concrete prompt-design principles emerged from the study?\\n4. What limitations remain despite these gains?\\nWe conclude with actionable guidelines for practitioners.\\n6.1\\nManual Prompt–Compression vs. Autonomous\\nAgent Debugging\\nLarge multi-turn agents (e.g., SWE-agent, GraphCodeBERT-agent) can autonomously\\nexplore, test, and refine patches, often converging without human guidance. However,\\nthey pay in **latency, token-usage, and model cost**, and still lack deep semantic aware-\\nness of domain-specific bugs. Our compression pipeline targets the complementary corner\\nof the design space: high human insight + minimal model calls. Table 6.1 summarises\\nthe trade-offs.\\nTable 6.1: Qualitative comparison of two debugging paradigms.\\nDimension\\nIterative Agent\\nOne-shot Compression\\nLLM\\ncalls\\n(is-\\nsue 12907)\\n∼20 GPT-4o requests\\n1−2 Qwen-72B requests\\nHuman effort\\nLow (setup)\\nHigh (craft prompt)\\nMonetary cost1\\n$0.40–0.70\\n$0.31–0.35\\nBug-localisation\\nskill\\nOptional\\nRequired\\nTurn-around\\ntime\\nMinutes–hours\\nSeconds (after prompt)\\nRobustness\\nto\\nunknown bugs\\nHigh\\nModerate\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 38}, page_content='Key insight.\\nWhen a developer already understands the failing logic, a hand-crafted,\\nhighly focused prompt can outperform an agent both in time and cost. Conversely, for\\nmulti-module or poorly localized failures, the agent’s breadth search is safer.\\n6.2\\nSemantic Clarity Improves Success Rate\\nSection 5.2 showed that semantic precision—not merely code context—governs success.\\n• astropy-12907 (Listing 5.4) supplied the entire function, allowing the model to\\ninfer that the right-hand block should copy right rather than overwrite with 1.\\n• astropy-14995 (Listings 5.5 – 5.6) kept the same code body in both prompts;\\nonly the wording that emphasised “operand.mask vs.\\noperand” differed. The ex-\\nplicit version passed, the vague one failed.\\n• astropy-6938 (Section 5.3) demonstrated that even when the correct line (output_field\\n,→[:]=...) is known, how we present the instruction matters: # headings sup-\\npressed attention, while bullet points preserved it.\\nHence, human authorship is still indispensable for injecting the semantic essence of\\nthe bug—something current LLMs do not infer reliably from code alone.\\n6.3\\nCost & Latency Trade-offs Revisited\\nSection 4.4 quantified API costs; here we interpret those numbers:\\n1. Token-level economics.\\nOne-shot compression saved ∼25–35% of token cost\\nrelative to iterative GPT-4o, but these savings disappear if human labour is billed.\\n2. Wall-clock delay. After the prompt template stabilised, a single Qwen-72B call\\n(∼15 s) replaced 20 GPT-4o rounds (>3 min). This matters for CI pipelines with\\nstrict timeouts.\\n3. Human–computer interaction.\\nCrafting the compressed prompt dominated\\nearly experiments (5 h per issue); a reusable template later reduced this to 3 h, but\\ncost advantage remains marginal if developer time is valued at industry rates.\\n6.4\\nPractical Guidelines\\n[label=0.]Use one-shot compression only after localising the bug. If root\\ncause is unknown, start with an agent or traditional testing. Keep all control-flow\\nlines; trim comments selectively. Structural omissions were the primary source\\nof failure (Section 5.2). State the fix in one crisp declarative sentence. E.g.,\\n“Replace in-place constant 1 by right.” Avoid markdown #-headers for ac-\\ntionable steps; use bullet lists + bold tokens instead. Pin diff hunk lines\\nto avoid offset errors, and run a trailing-space fixer (Listing 4.4).\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 39}, page_content='6.5\\nLimitations\\n1.2.3.4.5.• Dataset freshness. Section 4.1.1 showed that 63 % of official SWE-bench-Lite\\npatches can no longer be replayed cleanly because the underlying code has evolved.\\nSuch drift can mis-classify a correct patch as Error.\\n• Domain expertise required. Authors must understand the bug well enough to\\ncraft a semantically precise hint—contrary to the “black-box autopilot” vision.\\n• Model brittleness.\\nSmall models ignored do-not-snip directives; multilingual\\nvariants produced Russian text, revealing sensitivity to unseen token patterns.\\n• Dataset skew. Section 4.3.4 showed that stale benchmark snapshots can falsely\\nmark correct patches as failures.\\n• Scalability. Manual compression does not scale to thousands of issues without\\ntooling support (auto-template filling, diff linters, etc.).\\n6.6\\nTakeaways\\nHuman-guided compression and one-shot prompting excel when the\\nbug is well understood and the cost of human insight is justified\\nby runtime or token constraints.\\nFor exploratory debugging, au-\\ntonomous agents remain indispensable.\\nThis dichotomy suggests a hybrid future: agents provide broad search and automatic\\ntest loops, while developers intervene with carefully compressed prompts at high-leverage\\njunctures—leveraging strengths of both paradigms for efficient, reliable software mainte-\\nnance.\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 40}, page_content='Chapter 7\\nFuture Work\\nWhile our study demonstrates that carefully engineered prompt–compression pipelines\\ncan rival fully–automatic agents in cost and accuracy, several limitations remain. We\\noutline six concrete research directions that would further improve reliability, scalability,\\nand generality.\\n7.1\\nLearning–Based Importance Scoring\\nOur current compression relies on manual heuristics (e.g. keeping if branches and re-\\nmoving docstrings). A promising direction is to learn token–level importance:\\n• Perturbation-based saliency: Train a lightweight model that predicts the drop\\nin patch pass rate when a given line is masked.\\n• Reinforcement Learning (RL): Frame compression as an RL policy that deletes\\ntokens and receives SWE-bench pass/fail reward, similar to zhou2022less’s draft-\\nand-revise pipeline.\\n• Contrastive supervision: Use pairs of (good / bad) compressed prompts produced\\nin Section 5.2 as supervision for a BERT-style classifier that predicts “keep / prune.”\\n7.2\\nDynamic k-Beam Compression\\nInstead of sampling ten random compressions, we can perform a length–controlled beam\\nsearch that:\\n1. Minimises prompt length under the constraint LLM(prompt) →test pass.\\n2. Explores multiple minima to improve diversity, then ensembles the top–k candidates\\nduring patch generation.\\n7.3\\nTighter Feedback Loops with In-Context Evalu-\\nation\\nRecent work on “toolformer”–style agents shows that an LLM can call unit tests inside\\nthe same context window. Embedding a tiny subset of SWE-bench tests directly in the\\nprompt would create an inner feedback loop, allowing the model to iteratively refine\\npatches without external invocations—potentially reducing both latency and API cost.\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 41}, page_content='7.4\\nRobust Dataset Versioning\\nSection 4.3.4 revealed silent mismatches between our local checkout and the official\\nswe-bench snapshot. Future work should:\\n• Integrate git submodules pointing to exact commit hashes of each buggy version;\\n• Provide a CLI that hashes every source file SHA256(f) prior to evaluation and\\naborts if checksums diverge;\\n• Publish a Test Vectors suite so that third-party researchers can sanity-check envi-\\nronment parity in one command.\\n7.5\\nCross-Repository Generalisation\\nAlthough our experiments already span several repositories within SWE-bench Lite,\\nthey still concentrate on Python-centric projects. To extend the pipeline to truly large-\\nscale and multi-language codebases (e.g. TensorFlow, PyTorch, Kubernetes), we identify\\nthree open challenges:\\n1. Vocabulary adaptation for domain-specific APIs and idioms that rarely appear\\nin scientific-Python projects;\\n2. Prompt templates tailored to languages beyond Python (e.g. C++, Go, Java),\\nincluding build-system nuances and header-file layouts;\\n3. Empirical validation of whether the markdown-formatting effects discovered in\\nSection 5.3 replicate across languages and testing frameworks.\\n7.6\\nEvaluating the Qwen 3 Model Family\\nOn 29 April 2025 Alibaba released Qwen 3, an open-weight model family ranging from\\n0.6 B to 235 B parameters, with two MoE variants that activate only a fraction of their\\ntotal weights (3 B and 22 B respectively) while matching or surpassing dense models\\non coding benchmarks[20, 21]. Early public tests report that the 30 B-A3B MoE model\\nachieves coding accuracy close to qwen2.5-72B-instruct at roughly ∼60 % lower GPU\\nFLOPs[22].\\nWhy it matters for prompt compression.\\nLower activation cost means we can\\nraise sampling temperatures or beam widths without breaching latency or budget limits,\\nenabling a larger candidate set per compressed prompt (cf. Section 7.2). In addition,\\nQwen 3 introduces a “hybrid thinking mode” toggle that lets users choose between step-\\nby-step reasoning and fast generation, potentially opening novel compression–generation\\nschedules.\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 42}, page_content='Proposed experiments.\\n1. Cost/quality curve.\\nRe-run our Astropy and Django pipelines with Qwen3-\\n30B-A3B and Qwen3-4B to measure token savings and patch pass rate against\\nQwen2.5-72B (baseline)[23].\\n2. MoE-aware compression. Investigate whether masking less-attended tokens in\\nthe prompt leads to sparser expert activation, further reducing runtime cost.\\n3. Hybrid-mode scheduling. Evaluate a two-phase decode: first in “non-thinking”\\nmode for breadth, then switch to “thinking” mode only for top-k candidates that\\ncompile.\\nIf Qwen 3 delivers comparable accuracy at substantially lower inference costs, it could\\nbecome the default large model in our pipeline, pushing overall API spend below $0.20\\nper issue while maintaining or improving success rates.\\nOutlook\\nIn summary, the next generation of LLM-assisted debugging systems will be adaptive—learning\\nwhich context truly matters, interactive—with tight inner loops for self-evaluation, and\\nrepository-agnostic—scaling beyond a single benchmark. We hope the empirical findings\\nand open-sourced artifacts of this study serve as a foundation for that endeavour.\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 43}, page_content='Bibliography\\n[1] Santiago J. Miret Jimenez, Jialun Li, Aman Madaan, Anshul Tiwari, Shafiq Joty,\\nMartin M¨uller, and Graham Neubig. Swe-bench: Can language models resolve real-\\nworld github issues? arXiv preprint arXiv:2310.06770, 2023.\\n[2] Yilun Liu, Santiago J. Miret Jimenez, Jiayi Feng, Graham Neubig, and Yongjie Jes-\\nsica Zhang. Seaview: An interactive visual analytics system for software engineering\\nagents. arXiv preprint arXiv:2405.15793, 2024.\\n[3] OpenAI. How much does gpt-4 cost?, 2023. Accessed: 2024-05-04.\\n[4] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua:\\nCompressing prompts for accelerated inference of large language models. Microsoft\\nResearch Blog, 2023. Accessed: 2024-05-04.\\n[5] Shubham Shandilya, Mengzhou Xia, Souvik Ghosh, Huiqiang Jiang, Jiawei Zhang,\\nQianhui Wu, and Victor R¨uhle. Taco-rl: Task aware prompt compression optimiza-\\ntion with reinforcement learning. arXiv preprint arXiv:2409.13035, 2024. Accessed:\\n2024-05-04.\\n[6] MongoDB. How to optimize llm applications with prompt compression using mon-\\ngodb, 2024. Accessed: 2024-05-04.\\n[7] PromptOpti. Prompt compression tool for llms: Optimize and secure prompts, 2024.\\nAccessed: 2024-05-04.\\n[8] Ambarish Deshpande. Prompt compression: Budget friendly rag overload manage-\\nment, 2023. Accessed: 2024-05-04.\\n[9] Tobias Schnabel and Jennifer Neville. Symbolic prompt program search: A structure-\\naware approach to efficient compile-time prompt optimization.\\narXiv preprint\\narXiv:2404.02319, 2024. Accessed: 2024-05-04.\\n[10] Zongqian Li et al. Prompt compression for large language models: A survey. arXiv\\npreprint arXiv:2403.15050, 2024.\\n[11] Ajay Nagle et al. Rate-distortion theory for prompt compression in large language\\nmodels. arXiv preprint arXiv:2404.06334, 2024.\\n[12] Huiqiang Jiang et al. Llmlingua: Compressing prompts for accelerated inference of\\nlarge language models. arXiv preprint arXiv:2310.05736, 2023.\\n[13] Zhuoshi Pan et al.\\nLlmlingua-2: Data distillation for efficient and faithful task-\\nagnostic prompt compression. arXiv preprint arXiv:2403.12968, 2024.\\n[14] Shivam Shandilya et al. Taco-rl: Task aware prompt compression optimization with\\nreinforcement learning. arXiv preprint arXiv:2409.13035, 2024.\\n[15] Jinwu Hu et al. Dynamic compressing prompts for efficient inference of large language\\nmodels. arXiv preprint arXiv:2504.11004, 2025.\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T21:02:17+00:00', 'source': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'file_path': '..\\\\data\\\\pdf\\\\Final_year_project_Peter (8).pdf', 'total_pages': 45, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-29T21:02:17+00:00', 'trapped': '', 'modDate': 'D:20250429210217Z', 'creationDate': 'D:20250429210217Z', 'page': 44}, page_content='[16] Yue Zhang and Wei Li. How i learned to start worrying about prompt formatting,\\n2023. arXiv:2310.11324.\\n[17] Hana Lee and Taesun Kim.\\nEmojiprompt:\\nGenerative prompt obfuscation for\\nprivacy-preserving llms, 2024. arXiv:2402.05868.\\n[18] Mohit Sewak. Emoji jailbreaks: Are your ai models vulnerable? Google Cloud Blog,\\n2024.\\n[19] Ning Liu and Qian Chen. The impact of prompt block order on llm performance,\\n2024. arXiv:2406.09972.\\n[20] Qwen Team. Qwen3: Think deeper, act faster, 2025.\\n[21] Kyle Wiggers. Alibaba unveils qwen3, a family of ‘hybrid’ ai reasoning models, 2025.\\n[22] Piotr Macai. Alibaba released multimodal qwen 3, 2025.\\n[23] Qwen Contributors. Qwen/qwen3-30b-a3b on hugging face, 2025.\\n43'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 0}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n1\\nLecture 8\\nTop-Down Parsing\\nCOMP0012'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 1}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n2\\nReview\\n• We can specify language syntax using CFG\\n• A parser\\n– answers s \\uf0ce L(G)\\n– builds a parse tree\\n– passes it on to the rest of the compiler\\n• How do we answer s \\uf0ce L(G) and build a parse tree?'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 2}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n3\\nIntroduction to Top-Down Parsing\\n• Terminals are seen in their order of \\nappearance in the source code: \\n           t1  t2  t3  t4  t5\\n \\n• The parse tree is constructed\\n– From the top\\n– From left to right\\nA\\nt1\\nB\\nC\\nt2\\nD\\nt3\\nt5\\nt4'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 3}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n4\\nRecursive Descent Parsing\\n• Consider the grammar\\n      E → T + E | T\\n      T → int  | int * T | ( E )\\n• The input token stream is int * int\\n• Start with top-level non-terminal E\\n• Try the rules for E in order'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 4}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n5\\nRecursive Descent Parsing\\nTry the rules for E in order\\n• Which order?\\n• Each of E’s rules for one step?\\n– Exponential!\\n• Instead, we try each rule exhaustively, before \\npreceding to the next'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 5}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n6\\nRecursive Descent Parsing Example\\n1.\\nTry E1 → T1 + E2 \\nE1 →   T1 + E2\\n       int * int\\nGrammar: E → T + E | T\\n     T → int  | int * T | ( E )\\nInput:  int * int'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 6}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n7\\nRecursive Descent Parsing Example\\n2.\\nTry T1 → int1 \\nE1 → T1 + E2 → int1 + E2 \\n                                   ≠\\n                         int * int\\nGrammar: E → T + E | T\\n     T → int  | int * T | ( E )\\nInput:  int * int'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 7}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n8\\nRecursive Descent Parsing Example\\n3. Try T1 → int1 * T2, then T2 → int2 \\nE1 → T1 + E2 → int1 * T2 + E2 → int1 * int2 + E2 \\n                                                     int * int\\nGrammar: E → T + E | T\\n     T → int  | int * T | ( E )\\nInput:  int * int'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 8}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n9\\nRecursive Descent Parsing Example\\n4. Try T1 → (E)\\nE1 → T1 + E2 → (E3) + E2 \\n                            ≠\\n                         int * int\\nWe have exhausted the choices for T1.\\nGrammar: E → T + E | T\\n     T → int  | int * T | ( E )\\nInput:  int * int'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 9}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n10\\nRecursive Descent Parsing Example\\n5.\\nTry E1 → T1 \\n6.\\nTry T1 → int1 \\nE1 →   T1 →  int1\\n                      int * int\\n7.\\nTry T1 → int1 * T2\\nE1 →   T1 →  int1 * T2\\n                      int * int\\n8. Try T2 → int2\\nE1 →   T1 →  int1 * T2 → int1 * int2\\n                                         int * int\\nGrammar: E → T + E | T\\n     T → int  | int * T | ( E )\\nInput:  int * int'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 10}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n11\\nRecursive Descent Parsing Example\\nThe successful parse produces \\nthis parse tree.\\nE1\\nT1\\nint1\\n*\\nT2\\nint2\\nGrammar: E → T + E | T\\n     T → int  | int * T | ( E )\\nInput:  int1 * int2'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 11}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n12\\nRecursive Descent Parsing\\nFor a grammar G, to parse is to produce a parse tree for the \\ntoken sequence t1 t2 ⋯ tn.\\nDefinition [Recursive Descent Parsing]: From a grammar’s start \\nsymbol, try all the productions exhaustively in order.\\nFor A ∈ N, B ∈ (N ∪ T)* , the fringe of the parse tree is \\n \\nt1 t2 ⋯ tk AB \\nThe fringe is the prefix of a sentential form; t1 t2 ⋯ tk matches a \\nprefix of the input.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 12}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n13\\nRecursive Descent Parsing\\n• Definition [Recursive Descent Parsing]: From a grammar’s start \\nsymbol, try all the productions exhaustively.\\n– The fringe of the parse tree is t1 t2 … tk A\\n– Try each production for A exhaustively, only moving to A’s next \\nproduction when there is a mismatch. \\n• If A → BC is a production, the new fringe is t1 t2 … tk BC\\n• When B = w1 w2 … wk D, the fringe is extended to t1 t2 … tk w1 w2 … wk DC\\n– Backtrack when the fringe does not match the length k prefix of \\nthe input \\n– Stop when the input is matched or there are no more non-\\nterminals'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 13}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n18\\nWhen Recursive Descent Does Not Work\\n• Consider a production X → X\\uf061.\\n– What goes wrong when we try this rule during parsing?\\n– It produces no fringe!\\n– We reach it when nothing else matched before it under the \\ncurrent ordering, so, once we reach it, we cannot escape it.\\n• A left-recursive grammar has a production\\n           X →+ X\\uf061   for some \\uf061\\n• Recursive descent parsing diverges on left-recursive grammars'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 14}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n19\\nLeft Recursion Elimination\\nConsider the left-recursive grammar G = S → S\\uf061 | \\uf062\\nL(G) generates all strings starting with a \\uf062 and followed \\nby any number of \\uf061. \\nNote: \\uf061 ≠ \\uf065, since S → S is useless.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 15}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n20\\nElimination of Left Recursion\\nConsider the left-recursive grammar S → S\\uf061 | \\uf062\\nConsider the input \\uf062\\uf061\\uf061 ∈ L(S).  \\nRecursive descent works if we reorder the rules to try \\uf062 first (i.e. \\nS → \\uf062 | S\\uf061):\\n \\nS → \\uf062 \\n → S\\uf061 → \\uf062\\uf061 \\n → S\\uf061\\uf061 → \\uf062\\uf061\\uf061 ✓\\ndenotes a mismatch, triggering a backtrack.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 16}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n21\\nElimination of Left Recursion\\nConsider the left-recursive grammar S → S\\uf061 | \\uf062\\nWhat goes wrong when we use it to parse inputs under recursive \\ndescent?\\nConsider the input \\uf061\\uf061 ∉ L(S).'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 17}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n22\\nElimination of Left Recursion\\nConsider the left-recursive grammar S → S\\uf061 | \\uf062\\nWhat goes wrong when we use it to parse inputs under recursive \\ndescent?\\nConsider the input \\uf061\\uf061 ∉ L(S).\\n \\nS → S\\uf061 → S\\uf061\\uf061 → S\\uf061\\uf061\\uf061 → ⋯\\nThis produces no fringe to match.\\nCan we fix this problem?'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 18}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n23\\nElimination of Left Recursion\\nConsider the left-recursive grammar S → S\\uf061 | \\uf062\\nL(S) generates all strings starting with a \\uf062 and followed \\nby any number of \\uf061. \\nNote: \\uf061 ≠ \\uf065;  S → S is useless.\\nYes!  We can rewrite it to use right-recursion:\\n                 S → \\uf062S’\\n                 S’ → \\uf061S’ | \\uf065'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 19}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n24\\nElimination of Left-Recursion Example\\nThe grammar\\n    S → 1 | S0 \\n( \\uf062 = 1 and \\uf061 = 0 )\\ncan be rewritten \\n   S → 1S’\\n     S’ → 0S’ | \\uf065'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 20}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n25\\nElimination of Left-Recursion\\nFor \\uf061i ∈ (N ∪ T)+, \\uf062i, 𝛾 ∈ (N ∪ T)*, rewrite \\n S → S\\uf0611 | … | S\\uf061n | \\uf0621 | … | \\uf062m\\nto\\n S → \\uf0621S’ | … | \\uf062mS’\\nS’→ \\uf0611S’ | … | \\uf061nS’ | \\uf065\\nwhere \\uf062i ≠ S𝛾.\\nAll strings derived from S start with one of \\uf0621, …, \\uf062m and continue \\nwith instances of \\uf0611, …, \\uf061n.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 21}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n27\\nGeneral Left Recursion:  Handling Indirect Recursion\\n• The grammar \\n         S → A\\uf061 | \\uf064\\n         A → S\\uf062\\n is also left-recursive since\\n                S →+ S\\uf062\\uf061\\n• This indirect left-recursion must also be eliminated\\n– First substitute A into S to form S → S\\uf062\\uf061 | \\uf064 then \\neliminate the direct left-recursion.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 22}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n28\\nGeneral Recursion:  Handling Mutual Recursion\\n• The grammar \\n         A → B | c\\n         B → A | d\\n may cause a parse to diverge: \\n                A → B → A → B →  A \\n• Such mutual recursion should also be eliminated\\n– Use substitution: A → A | d | c, then eliminate the useless \\nA → A, giving A → d | c.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 23}, page_content='General Recursion:  ε-masked Left Recursion\\nDefinition:  The nonterminal N is nullable when N →+ ε.\\nS → YS | q\\nY → AxB | Cc | cd | ε\\nThis grammar is left recursive because Y is nullable.\\nIn practice, we try to eliminate all ε rules to unmask lurking left \\nrecursion. \\nDr. Barr   COMP0012 Top-Down Parsing\\n29'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 24}, page_content='General Recursion:  Eliminating ε Rules\\nS → YS | q\\nY → AxB | Cc | cd | ε\\nThis grammar is left recursive because Y is nullable.  \\nAs with indirect recursion, substitute (inline) Y into S, forming  \\n \\nS → AxBS | CcS | cdS | q\\nWe drop the S → S generated from substituting Y → ε because it \\nis useless.\\nHow big is the resulting grammar?\\nDr. Barr   COMP0012 Top-Down Parsing\\n30'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 25}, page_content='General Recursion:  ε Elimination\\nConsider nullable X ∈ N. Let X have m rules in R and \\n \\nl → r = … X1 … X2 … Xk … ∈ R \\nhave the largest number of appearances of X in its RHS.  \\nEach appearance of X in r must replaced, using \\nsimultaneous substitution, with all permutations — \\nwith replacement — of X’ rules, producing mk new \\nrules.\\nWorst case exponential blow up in the number of rules.\\nDr. Barr   COMP0012 Top-Down Parsing\\n31'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 26}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n32\\nGeneral Left Recursion\\nRepeat\\n  G’ := G\\n  In G, eliminate\\n1. Unreachable rules and useless productions X → X \\n2. Direct left recursion via transformation\\n3. Indirect left recursion via substitution\\n4. Mutual recursion via substitution\\n5. \\uf065-masked recursion via substitution\\nuntil G’ = G // no elimination applies\\nSee https://en.wikipedia.org/wiki/Left_recursion'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 27}, page_content='Left Associativity & Left Recursion Elimination\\nConsider this left recursive grammar\\n    S → S + int | int\\nThe operator + is left associative.\\nFor example, the parse tree for 1 + 2 + 3 is:\\nSo 1 + 2 + 3 = ((1) + 2) + 3\\nDr. Barr   COMP0012 Top-Down Parsing\\n33\\n3\\nS\\n+\\nS\\n2\\n+\\nS\\n1\\nWhat if we eliminate its left recursion?'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 28}, page_content=\"Left Associativity & Left Recursion Elimination\\nThe grammar becomes:\\n    S → int S'\\n    S' → + int S’| ε\\nNow the parse tree for 1 + 2 + 3 is:\\nSo 1 + 2 + 3 = 1 (+ 2 (+ 3))\\n+ is no longer left associative!\\nDoesn’t matter when + denotes addition, but what about subtraction?\\nImagine replacing + with O in the grammar and adding the rule O → +| - ?\\nDr. Barr   COMP0012 Top-Down Parsing\\n34\\nS’\\nS\\n1\\n2\\n+\\nS’\\n3\\n+\\nS’\\nε\"),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 29}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n35\\nSummary of Recursive Descent\\n• Simple and general parsing strategy\\n– Left-recursion must be eliminated first\\n• Unpopular because of backtracking\\n– Thought to be too inefficient\\n– Outdated conventional wisdom?\\n• Often, we can avoid backtracking …'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 30}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n36\\nDefinition of First Sets \\n \\n \\n \\nFor G = (N,T,S,R), let X ∈ N \\uf0c8 T\\nFirst[X] = Ft[X] = { X | X ∈ T } \\n \\n  \\n \\n\\uf0c8 { t ∈ T | X ∈ N ∧ X →+ t\\uf061 } \\n \\n  \\n \\n\\uf0c8 { \\uf065 | X →+ \\uf065 }\\nFt[X] ⊆ T\\uf065'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 31}, page_content='Computing First Set on Sentential Forms:  Functional\\nFor α ∈ (N \\uf0c8 T), \\uf062 ∈ (N \\uf0c8 T)*, let\\n \\nhead(α \\uf062) = α\\n \\ntail(α \\uf062) = \\uf062\\nLet X ∈ (N \\uf0c8 T)+ // Lifting X to sentential forms\\nFt(X) = Ft(head(X)) \\uf0c8 { Ft(tail(Y)) | \\uf065 ∈ head(X) }\\nDr. Barr   COMP0012 Top-Down Parsing\\n37'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 32}, page_content='TODO:  visualize recursion\\nS → ABC\\nFt(S) = Ft(ABC) \\n \\n= Ft(A) if A is not nullable.\\n \\n= Ft(A) \\uf0c8 Ft(B) if A is nullable and B is not\\n \\n= Ft(A) \\uf0c8 Ft(B) \\uf0c8 Ft( C) \\n \\n \\nif A and B are nullable and C is not nullable\\n \\n= Ft(A) \\uf0c8 Ft(B) \\uf0c8 Ft( C) \\uf0c8 {\\uf065}\\n \\n \\nif A, B, and C are nullable\\nDr. Barr   COMP0012 Top-Down Parsing\\n38'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 33}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n39\\nComputing First Sets: Imperative Initialization \\nFor G = (N,T,S,R), Ft(X): \\n1.  forall t ∈ T\\n2.         Ft[t] := {t}\\n3.  forall X ∈ N\\n4.         Ft[X] := ∅\\nNOTE: Ft[X] ≠ Ft(X)'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 34}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n40\\nComputing First Sets: Imperative \\n \\nFor G = (N,T,S,R), Ft(X): \\n5.  forall productions r = X → A1 … An ∈ R:\\n6.       for each Ai in order:\\n7.              Ft[X] := Ft[X] ⋃ (Ft(Ai) – {\\uf065})\\n8.            next r if \\uf065 \\uf0cf Ft(Ai)\\n9.       // Only reached if all Ai in r are nullable\\n10.       Ft[X] := Ft[X] \\uf0c8 {\\uf065}'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 35}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n41\\nFirst Sets Example  \\n \\n \\n \\n• Consider the grammar \\n    E → T X                               X → + E | \\uf065 \\n    T → ( E ) | int Y                  Y → * T | \\uf065\\n• First sets'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 36}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n42\\nFirst Sets Example  \\n \\n \\n \\n• Consider the grammar \\n    E → T X                               X → + E | \\uf065 \\n    T → ( E ) | int Y                  Y → * T | \\uf065\\n• First sets\\n      Ft( ( ) = { ( }\\n      Ft( ) ) = { ) }\\n      Ft( int ) = { int }\\n      Ft( + ]) = { + }\\n      Ft( * ) = { * }'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 37}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n43\\nFirst Sets Example  \\n \\n \\n \\n• Consider the grammar \\n    E → T X                               X → + E | \\uf065 \\n    T → ( E ) | int Y                  Y → * T | \\uf065\\n• First sets\\n      Ft( ( ) = { ( }\\n      Ft( ) ) = { ) }\\n      Ft( int ) = { int }\\n      Ft( + ) = { + }          \\n      Ft( * ) = { * } \\nFt( E ) = Ft( T ) = { int, ( }\\nFt( T ) = { int, ( }\\nFt( X ) = { +, \\uf065 }\\nFt( Y ) = { *, \\uf065 }'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 38}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n44\\nDefinition of Follow Sets\\n       Follow[X] = Fw[X] = \\n                           { t | Y → \\uf061X\\uf062 ∈ R ∧ t ∈ Ft(\\uf062) – {\\uf065} } \\uf0c8\\n \\n \\n       \\n   { t | Y → \\uf061X\\uf062 ∈ R ∧ \\uf065 ∈ Ft(\\uf062) ∧ t ∈ Fw(Y) }\\n \\n \\n \\nFw(X) ⊆ T$\\n \\n Fw(X) is the smallest such set.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 39}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n45\\nComputing Follow Sets:  Initialization\\n1.\\nCompute the First sets for all non-terminals.\\n2.\\nforall 𝑋∈𝑁, Fw(X) := ∅    // G = (N,T,S,R)\\n3.\\nFw[S] := {$}'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 40}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n46\\nComputing Follow Sets  \\n \\nFw(X):\\n1.\\nforall productions r = Y → …X A1…An ∈ R:\\n2.\\n      forall Ai in order:\\n3.\\n            Fw[X] := Fw[X] \\uf0c8 (Ft(Ai) – {\\uf065})\\n4.\\n            next r if \\uf065 \\uf0cf Ft(Ai)\\n5.\\n      // Only reached if all Ai in r are nullable\\n6.\\n      Fw[X] := Fw[X] \\uf0c8 Fw(Y)'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 41}, page_content='Fw(X) Observations\\n1. \\uf065 \\uf0cf Fw[X]; if not, one cannot know when to discard a \\nnonterminal; in contrast, $ \\uf0cf Ft[X].\\n2. The outer loop iterates on productions that contain \\nX in their RHS, not in their LHS, in contrast with \\ncomputing the First.\\n3. When X appears on the RHS of a production with a \\nnullable suffix, it can be followed by whatever can \\nfollow Y.\\nDr. Barr   COMP0012 Top-Down Parsing\\n47'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 42}, page_content='Fw(X):  The Tricky Bit\\nFw(X) = \\n    { t | Y → \\uf061X\\uf062 ∈ R ∧ t ∈ Ft(\\uf062) – {\\uf065} } \\uf0c8 \\n    { t | Y → \\uf061X\\uf062 ∈ R ∧  \\uf065 ∈ Ft(\\uf062) ∧ t ∈ Fw(Y) }\\nDr. Barr   COMP0012 Top-Down Parsing\\n48\\nWhen X appears on the \\nRHS of a production with a \\nnullable suffix, it can be \\nfollowed by whatever can \\nfollow Y.  Why is this?\\n⋯ Y I l\\n         ↓    Y → \\n\\uf061X\\uf062\\n⋯ \\uf061X\\uf062 I l\\n         ↓   \\uf062 → \\uf065\\n⋯ \\uf061X I l\\nl ∈ T is the lookahead, \\nthe next token to match \\nin the input.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 43}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n49\\nFollow Sets Example\\nRecall the grammar \\n    E → T X                               X → + E | \\uf065 \\n    T → ( E ) | int Y                  Y → * T | \\uf065\\n \\nNonterminal\\nFollow\\nE\\n{ $ } \\uf0c8 { ) } \\uf0c8 Fw(X)\\nX\\nT\\nY'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 44}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n50\\nFollow Sets Example\\nRecall the grammar \\n    E → T X                               X → + E | \\uf065 \\n    T → ( E ) | int Y                  Y → * T | \\uf065\\n \\nNonterminal\\nFollow\\nE\\n{ $ } \\uf0c8 { ) } \\uf0c8 Fw(X)\\nX\\nFw(E) =\\nT\\nY'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 45}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n51\\nFollow Sets Example\\nFw(E) = { $, ) } \\uf0c8 Fw(X) \\nFw(X) = Fw(E) \\n \\n⇒\\nFw(E) = { $, ) } \\uf0c8 Fw(E) \\nWhat is the smallest set over 2𝑇$ that \\nsatisfies this constraint?\\nFw(E) = T = { $, ) } \\uf0c8 T = T \\n, but maximal \\nFw(E) = ∅ ≠ ({ $, ) } \\uf0c8 ∅)  = { $, ) } \\nFw(E) = { $, ) } = { $, ) } \\uf0c8 { $, ) } \\nCan we go smaller? \\nT$ – {t}, t ∈ T$\\n{ $, ) }\\nT$ = { $, (, ), int, +, * }\\n∅\\nhttps://en.wikipedia.org/wiki/Hasse_diagram'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 46}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n52\\nFollow Sets Example\\nRecall the grammar \\n    E → T X                               X → + E | \\uf065 \\n    T → ( E ) | int Y                  Y → * T | \\uf065\\n \\nNonterminal\\nFollow\\nE\\n{ $ } \\uf0c8 { ) } \\uf0c8 Fw(X) = Fw(E) \\uf0c8 { $, ) }\\nX\\nFw(E) \\nT\\nY'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 47}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n53\\nFollow Sets Example\\nRecall the grammar \\n    E → T X                               X → + E | \\uf065 \\n    T → ( E ) | int Y                  Y → * T | \\uf065\\n \\nNonterminal\\nFollow\\nE\\n{ $ } \\uf0c8 { ) } \\uf0c8 Fw(X) = Fw(E) \\uf0c8 { $, ) } = { ), $ }\\nX\\nFw(E) = { ), $ }\\nT\\nY'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 48}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n54\\nFollow Sets Example\\nRecall the grammar \\n    E → T X                               X → + E | \\uf065 \\n    T → ( E ) | int Y                  Y → * T | \\uf065\\n \\nNonterminal\\nFollow\\nE\\n{ $ } \\uf0c8 { ) } \\uf0c8 Fw(X) = Fw(E) \\uf0c8 { $, ) } = { ), $ }\\nX\\nFw(E) = { ), $ }\\nT\\nFw(Y) \\uf0c8\\nY'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 49}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n55\\nFollow Sets Example\\nRecall the grammar \\n    E → T X                               X → + E | \\uf065 \\n    T → ( E ) | int Y                  Y → * T | \\uf065\\n \\nNonterminal\\nFollow\\nE\\n{ $ } \\uf0c8 { ) } \\uf0c8 Fw(X) = Fw(E) \\uf0c8 { $, ) } = { ), $ }\\nX\\nFw(E) = { ), $ }\\nT\\nFw(Y) \\uf0c8 Ft(X) – { \\uf065 } \\uf0c8 Fw(E)\\nY\\nBecause X is nullable.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 50}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n56\\nFollow Sets Example\\nRecall the grammar \\n    E → T X                               X → + E | \\uf065 \\n    T → ( E ) | int Y                  Y → * T | \\uf065\\n \\nNonterminal\\nFollow\\nE\\n{ $ } \\uf0c8 { ) } \\uf0c8 Fw(X) = Fw(E) \\uf0c8 { $, ) } = { ), $ }\\nX\\nFw(E) = { ), $ }\\nT\\nFw(Y) \\uf0c8 Ft(X) – { \\uf065 } \\uf0c8 Fw(E) = Fw(Y) \\uf0c8 { + } \\uf0c8 { ), $ } \\nY\\nFw(T)'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 51}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n57\\nFollow Sets Example\\nRecall the grammar \\n    E → T X                               X → + E | \\uf065 \\n    T → ( E ) | int Y                  Y → * T | \\uf065\\n \\nNonterminal\\nFollow\\nE\\n{ $ } \\uf0c8 { ) } \\uf0c8 Fw(X) = Fw(E) \\uf0c8 { $, ) } = { ), $ }\\nX\\nFw(E) = { ), $ }\\nT\\nFw(Y) \\uf0c8 Ft(X) – { \\uf065 } \\uf0c8 Fw(E) = Fw(Y) \\uf0c8 { + } \\uf0c8 { ), $ } \\n= Fw(T) \\uf0c8 { ), $, + }\\nY\\nFw(T)'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-02-10T11:00:53+00:00', 'source': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'file_path': '..\\\\data\\\\pdf\\\\recursive-descent-fsets.pdf', 'total_pages': 53, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-10T11:00:53+00:00', 'trapped': '', 'modDate': \"D:20250210110053+00'00'\", 'creationDate': \"D:20250210110053+00'00'\", 'page': 52}, page_content='Dr. Barr   COMP0012 Top-Down Parsing\\n58\\nFollow Sets Example\\nRecall the grammar \\n    E → T X                               X → + E | \\uf065 \\n    T → ( E ) | int Y                  Y → * T | \\uf065\\n \\nNonterminal\\nFollow\\nE\\n{ $ } \\uf0c8 { ) } \\uf0c8 Fw(X) = Fw(E) \\uf0c8 { $, ) } = { ), $ }\\nX\\nFw(E) = { ), $ }\\nT\\nFw(Y) \\uf0c8 Ft(X) – { \\uf065 } \\uf0c8 Fw(E) = Fw(Y) \\uf0c8 { ), $ } \\uf0c8 { + }\\n= Fw(T) \\uf0c8 { ), $, + } = { ), $, + }\\nY\\nFw(T) = { ), $, + }'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-09-14T11:45:54+01:00', 'source': '..\\\\data\\\\pdf\\\\Summative Assessment 25-26.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Summative Assessment 25-26.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Derakhshanalavijeh, Roya', 'subject': '', 'keywords': '', 'moddate': '2025-09-14T11:45:54+01:00', 'trapped': '', 'modDate': \"D:20250914114554+01'00'\", 'creationDate': \"D:20250914114554+01'00'\", 'page': 0}, page_content='Summative Assessment for Governance and Control 2025/2026 \\nThe final mark of the module is composed of two parts: Group Presentation and Essay \\nwith 30% and 70% contribution to your final mark.  \\n \\nGroup Presentation In-Class (30% of the final mark): \\nAssignment Overview:  \\nYou will form groups of four and prepare a presentation that explores the link between \\ngovernance at different levels and project success in a detailed and well-structured \\nmanner. Make your groups before the second day of the module and announce them \\non Moodle.  \\nRead relevant papers on the relationship between governance and project success, meet \\nwith your team to plan the structure, and develop your presentation. It is recommended \\nthat you identify a case to illustrate your discussion and create debates around that case \\nto enrich your analysis. On the day of delivery, every group member must speak during \\nthe presentation. \\nAssignment Requirements: \\n• Select a relevant project governance concept (e.g., risk management, decision-\\nmaking frameworks, stakeholder engagement, trust, ethics, etc.) and connect it \\nto project success. \\n• Optional: Identify or develop a case that illustrates governance in practice and \\nuse it to generate discussion and debate during your presentation. \\n• Critically evaluate the selected concept using theory and evidence from \\nacademic literature as well as examples from industry. \\n• Explain how governance (at each level applicable) impacts project success.  \\n• Demonstrate effective teamwork, with all group members contributing to both the \\npreparation and the delivery of the presentation. \\n• Deliver a presentation that is clear, engaging, and well structured, with all \\nmembers speaking during the session.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-09-14T11:45:54+01:00', 'source': '..\\\\data\\\\pdf\\\\Summative Assessment 25-26.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Summative Assessment 25-26.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Derakhshanalavijeh, Roya', 'subject': '', 'keywords': '', 'moddate': '2025-09-14T11:45:54+01:00', 'trapped': '', 'modDate': \"D:20250914114554+01'00'\", 'creationDate': \"D:20250914114554+01'00'\", 'page': 1}, page_content='Marking Criteria: \\nCriteria 1: Depth of Analysis and Critical \\nEvaluation  \\nDemonstrates critical understanding of the \\nselected governance concept, supported by \\nacademic literature and industry evidence, \\nwith insightful evaluation rather than \\ndescription. \\n30% \\nCriteria 2: Use of Theory and Frameworks  \\nApplication of relevant theoretical \\nframeworks discussed in the module, \\nshowing understanding of governance \\nprinciples, paradigms, and theories. \\n30% \\nCriteria 3: Presentation Quality and \\nCommunication  \\nClarity, structure, time management, and \\nengagement of the presentation, use of \\nvisual aids where appropriate, and \\ncontribution by all team members. \\n20% \\nCriteria 4: Teamwork and Collaboration  \\nEvidence of effective group collaboration, \\nwith balanced participation and integration \\nof diverse perspectives. \\n20% \\n \\nEssay (70% of the final mark): \\nAssignment Overview: \\nThe assignment requires you to write a 2,000-word essay (including tables and figures), \\nfocusing on a concept relevant to project governance and its impact on the strategic \\nmanagement of projects. You will critically evaluate the selected concept and explore \\nits theoretical and practical application within an industry of your choice (or at a generic \\nlevel without specifying an industry). The essay should draw on relevant theories \\ndiscussed in Governance & Control, demonstrate the ability to apply these concepts \\nrelationally. All sources and references should be acknowledged using the APA \\nreferencing system.  \\nThe assignment should also briefly include practical insights or recommendations for \\nproject managers. Please remember that the main aim of this essay is to theoretically \\nappraise the concepts and theories. So do not use much space for practical/managerial \\nrecommendations.  \\nAssignment Requirements:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-09-14T11:45:54+01:00', 'source': '..\\\\data\\\\pdf\\\\Summative Assessment 25-26.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Summative Assessment 25-26.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Derakhshanalavijeh, Roya', 'subject': '', 'keywords': '', 'moddate': '2025-09-14T11:45:54+01:00', 'trapped': '', 'modDate': \"D:20250914114554+01'00'\", 'creationDate': \"D:20250914114554+01'00'\", 'page': 2}, page_content='You are expected to: \\n• Select a relevant project governance concept (e.g., risk management, decision-\\nmaking frameworks, stakeholder engagement, trust, ethics, etc.). \\n• Critically evaluate the selected concept, using theory and evidence from both \\nacademic literature and industry examples. \\n• Apply the concept to the strategic management of projects in an industry of \\nyour choice, or across multiple industries (at a generic level). \\n• Draw theoretical frameworks to discuss how that specific concept is linked to (or \\ninfluences) the project success and strategic alignment of projects within the \\npermanent organization. \\n• In all sections of the essay, you must be aware of (and clarify) the discussions are \\nabout which level of governance (corporate governance, governance of projects \\nor project governance). \\n• Draw on theoretical frameworks we discussed during the module (e.g., \\nprinciples of governance, governance paradigms, governance theories, etc.).  \\n• Do not create superficial discussions. There is a need to see that you have studied \\nand critically reflected on relevant academic articles of the topic and that you \\nare aware whether and how that concept can impact project success and \\nstrategic alignment of projects through governance. Remember that not all \\nconcepts are academically proofed to have an impact on the success of projects. \\nSo you may need to take ‘the extra step’. More detailed are discussed during the \\n1st day of the module.  \\n• Discussions created through AI can be easily identified as they always lack depth \\nand nuances. Use AI only to enhance English writing.  \\nThe essay must be deep and nuanced. A nuanced discussion is one that explores a topic \\nin a detailed, thoughtful, and sophisticated way, acknowledging its complexity and the \\nvarious factors or perspectives involved. Instead of presenting issues in a binary or \\noversimplified manner, a nuanced discussion considers shades of meaning, context, and \\nsubtle differences in views or interpretations. Key features of such a discussion include: \\n1. Multiple Perspectives: It incorporates and respects different viewpoints, \\nrecognizing that an issue may have more than one valid angle. \\n2. Attention to Context: It considers contextual factors that influence the issue, \\nmaking the conversation more accurate and relevant. In the case of your \\nassignment these contextual factors could be (but are not limited to) the \\nfinancial condition of the corporate, the sector, complexity of the project, \\nstrategic direction of the corporate, etc.  \\n3. Avoidance of Generalizations: Rather than relying on broad or sweeping \\nstatements, it uses precise language to reflect the complexities of the subject.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-09-14T11:45:54+01:00', 'source': '..\\\\data\\\\pdf\\\\Summative Assessment 25-26.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Summative Assessment 25-26.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Derakhshanalavijeh, Roya', 'subject': '', 'keywords': '', 'moddate': '2025-09-14T11:45:54+01:00', 'trapped': '', 'modDate': \"D:20250914114554+01'00'\", 'creationDate': \"D:20250914114554+01'00'\", 'page': 3}, page_content=\"4. Recognition of Ambiguity: It acknowledges that some questions may not have \\nclear-cut answers and embraces uncertainty or open-ended conclusions where \\nnecessary. \\n5. Deep Analysis: It involves delving into the underlying causes or reasons behind \\npositions, rather than sticking to surface-level explanations. \\nIn essence, a nuanced discussion doesn't rush to conclusions but carefully examines the \\nsubtleties, allowing for a richer and more informed conversation. \\nSubmission Deadline: TBD \\nFormat: \\nThis assignment has a limit of 2,000 words (including tables and figures). All sources \\nand references should be acknowledged using the APA referencing system. Use \\nAssignment Cover Page for the first page of your essay (available on Moodle). \\nThere is a 10% leeway for the word limit: submissions that are either 10% over or under \\nthe word count won’t be penalised.   \\n \\nType of content \\nCounts towards the word \\nlimit \\nTable of contents \\nYes \\nReference list or bibliography at the end \\nNo \\nCover page \\nNo \\nDiagrams, annotated pictures, figures and \\nany other visuals \\nYes \\nAppendices \\nNo \\nAbstract \\nYes \\nAcknowledgements \\nNo \\nFootnotes \\nYes \\nTables in the main text \\nYes \\nIn-text citations \\nYes \\n \\n Marking Criteria: \\n Criteria 1: an introduction that clearly sets \\nout the objectives of the essay and \\n10%\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-09-14T11:45:54+01:00', 'source': '..\\\\data\\\\pdf\\\\Summative Assessment 25-26.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Summative Assessment 25-26.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Derakhshanalavijeh, Roya', 'subject': '', 'keywords': '', 'moddate': '2025-09-14T11:45:54+01:00', 'trapped': '', 'modDate': \"D:20250914114554+01'00'\", 'creationDate': \"D:20250914114554+01'00'\", 'page': 4}, page_content='provides a guide through the rest of the \\npaper \\nCriteria 2: a well-written, comprehensive \\nresponse to the question set, that sets up \\nclear lines of argument, with high quality \\ncomparative analysis of the various \\naspects related to the exam question \\n50% \\nCriteria 3: high-quality conclusions that \\nbring together all the strands of the \\narguments made, and a clear and \\ndefinitive statement that summarises the \\nposition developed in the essay.  \\n20%  \\nCriteria 4: the use of relevant reference \\nsources, correctly cited, and with a \\nreasonably accurate reference list \\nprovided after the conclusion. (No marks \\nwill be deducted for errors in reference list \\npresentation.) \\n10%  \\nCriteria 5: the paper is legible, clearly \\nwritten, with correct syntax and grammar. \\n10%  \\n \\n \\nPenalties: \\n \\nPenalties \\n(as per UCL Academic Manual) \\n- \\nPenalties due to over-\\nlength cannot be more \\nthan 10% \\n \\n- \\nOver-length penalty \\ncannot take the student’s \\nmark below ‘Pass Mark’ \\n \\n- \\nIn the case the \\ncoursework that is \\nsubmitted is over-length \\nand is also late, the \\ngreater of any penalties \\nwill apply. \\n \\n- \\nAny use of AI that exceeds \\nthe permitted use in this \\nassessment brief will be'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-09-14T11:45:54+01:00', 'source': '..\\\\data\\\\pdf\\\\Summative Assessment 25-26.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Summative Assessment 25-26.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Derakhshanalavijeh, Roya', 'subject': '', 'keywords': '', 'moddate': '2025-09-14T11:45:54+01:00', 'trapped': '', 'modDate': \"D:20250914114554+01'00'\", 'creationDate': \"D:20250914114554+01'00'\", 'page': 5}, page_content=\"subject to UCL Academic \\nMisconduct policy and \\ncould lead to penalties. \\n \\nAssessment Support: \\nYou are encouraged to engage in class discussions as well as case study group works. \\nThe discussions will support you in better comprehending diverse aspects of governance. \\nThe module leader is present in all classes and can discuss any aspect of the module \\ncontent with you. \\nOne important support and feedback you can receive is on the last day of the module \\nwhen you will present the results of the group work on the influence of governance on \\nproject success. More details on this formative assessment are provided during the first \\nday of the module. \\nThe UCL Academic Communication Centre runs a free service offering workshops, \\ntutorials and support sessions to enhance academic writing and research skills. These \\nservices are available for undergraduate and postgraduate students in the Joint Faculties \\nof Arts & Humanities and Social & Historical Sciences, Maths & Physical Sciences, \\nEducation & Society and the Bartlett Faculty of the Built Environment:  \\nhttps://www.ucl.ac.uk/languages-international-education/ucl-academic-\\ncommunication-centre \\n \\nUCL Student Union English Language + Writing Support Programme supports non-native \\nEnglish-speaking students with their academic writing and speaking. Peer Tutors run \\nseveral different types of free activities to help you with your written and spoken English, \\nincluding a regular programme of workshops, one-to-one sessions and 'Coffee and \\nConversation' which is a weekly opportunity to get together and practice your spoken \\nEnglish \\nwith \\nother \\nstudents: \\nhttps://studentsunionucl.org/advice-and-\\nsupport/support/language-writing-support-programme\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "loader = DirectoryLoader(\"../data/pdf\", glob=\"*.pdf\", loader_cls= PyMuPDFLoader, show_progress=False)\n",
    "pdf_documents = loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "925ac637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
